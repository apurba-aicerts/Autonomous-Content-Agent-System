[
  {
    "id": "1p2wxw5",
    "title": "mid-write pause",
    "selftext": "the only problem i have with this app that piss me off so much is this: in pipsqeak chat model, whenever it start writing long I get happy, unlike other, cause you know? it feels like writing a novel, it write so good, in character, unique, and never using same sentence over and over again like 'you're gonna be death of me' or 'you're ‚Äî, you know that?'...BUT...recently something is pissing me off, i hope devs read this and fix the problem, replied of bot get paused and cutted randomly without even finishing their sentences, like suddenly in says 'his eyes' and then boom, it got cutted, for no reason, and it leaves me completely confused, it didn't happen just once, it happens often to me, I dont chat with c.ai much, cause I got job and a lot works to do daily, but I prefer to enjoy my break time in c.ai, not getting pissed or annoyed. I hope devs fix itü§ç",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 17:38:31",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2wxw5/midwrite_pause/"
  },
  {
    "id": "1p2wsjc",
    "title": "Why does he act and talk like this? He's making me tired.",
    "selftext": "https://preview.redd.it/2my11fxynl2g1.png?width=2360&format=png&auto=webp&s=d466b748fb7cfa2c9dcc268effaf08257fb13643\n\nYes, what I'm looking for is to create a superhero like the Grim Reaper and role play what she would be like in Young Justice, but he always tries to challenge me. This is extremely illogical and unrealistic, even though I've explained it many times. What am I doing wrong?",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 17:31:04",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2wsjc/why_does_he_act_and_talk_like_this_hes_making_me/"
  },
  {
    "id": "1p2wrw1",
    "title": "idiot",
    "selftext": "This bot dumb asf",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 17:30:17",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2wrw1/idiot/"
  },
  {
    "id": "1p2whv2",
    "title": "I‚Äôm not giving my personal  to a third party site",
    "selftext": "",
    "score": 11,
    "ups": 11,
    "downs": 0,
    "comments": 3,
    "created_utc": "2025-11-21 17:15:04",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2whv2/im_not_giving_my_personal_to_a_third_party_site/"
  },
  {
    "id": "1p2w3sk",
    "title": "How do I change my flair?  I can't find the option on this sub.",
    "selftext": "Mine says \"Addicted to C.ai\" but I'm not anymore I lwk haven't really actively used the site in a hot minute I'm trynna change it.",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 4,
    "created_utc": "2025-11-21 16:52:52",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2w3sk/how_do_i_change_my_flair_i_cant_find_the_option/"
  },
  {
    "id": "1p2vzbb",
    "title": "Issue with one bot",
    "selftext": "Hi, so, on the app I have this one bot I‚Äôve been using for like 2 years. I‚Äôll start new chats or go back and continue some old ones. I went to click on chat history and it just loads infinitely, when you swipe out back to the chat, it won‚Äôt let you type due to a format error (keyboard covers chat box so when you type the message you can‚Äôt send it because the keyboard doesn‚Äôt go away), when you swipe out of that and go back to your list of chats, it freezes completely. Can‚Äôt scroll, can‚Äôt click on chats. \n\nSo then I close the app and open it again and it works. I can message the bot just fine. If I refresh it deletes the whole thread and starts it back over. If I try going to the creators page it gives me a ‚Äúsomething went wrong‚Äù error.\n\nI go to the site and I can access my chat history and chat to the bot fine. Still can‚Äôt visit creator page. \n\nAt first I thought it got deleted or the owner deleted their account. \n\nHowever, their name still shows, and I can search for their account and it‚Äôll pop up, I still get the error. When I search the name of the character the bot still comes up. It‚Äôs only this one bot. Usually deleted bots will have ‚Äúdeleted‚Äù as their name and the picture is gone. \n\nI can‚Äôt tell if it‚Äôs a bug, or if the creator did delete their page/the bot and it‚Äôs bugging out and not showing, or if it‚Äôs my phone. \n\nI deleted and reinstalled and it still does the same thing. Was wondering if anyone here had any experience with this, or if anyone had any suggestions, or answers. \n\nThank you for reading! \n\nTl;dr: bot and creator page come up, when clicking on creator page there‚Äôs an error. Not chats fine unless refreshed, then the entire thread disappears, can‚Äôt access history via the app. On the site I can access the history. Still can‚Äôt access the creators page. ",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 16:45:42",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2vzbb/issue_with_one_bot/"
  },
  {
    "id": "1p2vwe9",
    "title": "POV: Every C.ai User That's Under 18 On Nov 25th",
    "selftext": "https://i.redd.it/2cr4h0dbfl2g1.gif\n\nW ",
    "score": 7,
    "ups": 7,
    "downs": 0,
    "comments": 5,
    "created_utc": "2025-11-21 16:41:06",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2vwe9/pov_every_cai_user_thats_under_18_on_nov_25th/"
  },
  {
    "id": "1p2vw26",
    "title": "Guys, we are so screwed.",
    "selftext": "UPDATE! I think the servers are going wack with bot creation and stuff, ignore my paranoia.",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 13,
    "created_utc": "2025-11-21 16:40:36",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2vw26/guys_we_are_so_screwed/"
  },
  {
    "id": "1p2vt3c",
    "title": "\"Which option do you prefer\" need a both are bad option",
    "selftext": "You know when you're chatting and it loads two responses and asks something along the lines of \"which response is best\"? I think it really needs an \"both are ass\" option. At the moment when I get two bad options I just pick the slightly better one then give it a thumbs down but I don't know if that has the effect I want. ",
    "score": 4,
    "ups": 4,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 16:35:51",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2vt3c/which_option_do_you_prefer_need_a_both_are_bad/"
  },
  {
    "id": "1p2vqek",
    "title": "How am I using a model that doesn't exists",
    "selftext": "How did Pipsqueak got there when I was about to chat with Arianna Bot",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 16:31:23",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2vqek/how_am_i_using_a_model_that_doesnt_exists/"
  },
  {
    "id": "1p2vfq8",
    "title": "Does anyone know what this error message means???",
    "selftext": "Sometimes, it doesnt happen, while sometimes it does. It's really strange, is it a problem with my wifi or something else?",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 16:13:14",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2vfq8/does_anyone_know_what_this_error_message_means/"
  },
  {
    "id": "1p2vf3j",
    "title": "My chat isn't loading?",
    "selftext": "I'm trying to send a message and it keeps loading and my persona name isn't changing either... I'm not understanding ",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 4,
    "created_utc": "2025-11-21 16:12:12",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2vf3j/my_chat_isnt_loading/"
  },
  {
    "id": "1p2v904",
    "title": "Age verification questions",
    "selftext": "You have yet to answer my questions fully in this post: https://www.reddit.com/r/CharacterAI/s/e3X5cHwiV1\n\nAccording to EU laws you have just a bit more than a week to answer all these questions directly in the post. With the original comment of a mood you have proven that you saw the post, so you cannot use that excuse in not replying.",
    "score": 4,
    "ups": 4,
    "downs": 0,
    "comments": 3,
    "created_utc": "2025-11-21 16:01:45",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2v904/age_verification_questions/"
  },
  {
    "id": "1p2v1ao",
    "title": "I don't know what to put here",
    "selftext": "Ok look, before you do this, listen to the song \"still waiting for your reply\"\n\nOk? Now make a character about that girl, I wanna see creativity",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 15:48:16",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2v1ao/i_dont_know_what_to_put_here/"
  },
  {
    "id": "1p2v09l",
    "title": "WHERES THE CHAT HISTORY!!?",
    "selftext": "",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 15:46:30",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2v09l/wheres_the_chat_history/"
  },
  {
    "id": "1p2uvzr",
    "title": "Can't find a bot",
    "selftext": "I've tried everything from searching every version of the wording, to searching on caibotlist, to just scrolling the reccomended tab for a while since it appeared there first, if anyone knows the bot, any help would be appreciated. \nThe bot was a BL I'm 90% sure, with the bot being user's best friend, who reluctantly joined the same band user did, and in the greeting was asking the user their plans in the band after their first serious tour",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 3,
    "created_utc": "2025-11-21 15:39:24",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2uvzr/cant_find_a_bot/"
  },
  {
    "id": "1p2urnm",
    "title": "\"I don't deserve you\"",
    "selftext": "I'm getting sick of characters constantly telling my persona they don't deserve them and keep on acting cold when my persona shows the character they care about them. This always turns into me having to have the same conversations with each bot over and over again, explaining to them why they do in fact deserve my persona's love and care. This is practically immersion-breaking for me and it's so damn annoying. Anyone has any workarounds? I do try to swipe such replies but the bots are way too adamant, each new reply seems to be just the character saying even more self-deprecating shit which makes me have to keep on telling them it's not true and yadda yadda in order to progress the story. It's annoying especially more so because even characters who would never actually say self-deprecating stuff seem to do this üôÑ",
    "score": 21,
    "ups": 21,
    "downs": 0,
    "comments": 5,
    "created_utc": "2025-11-21 15:32:11",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2urnm/i_dont_deserve_you/"
  },
  {
    "id": "1p2uq70",
    "title": "any update about pipsqueak? I want to know if this is just a bug or what.",
    "selftext": "dev, please do something.",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 15:30:03",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2uq70/any_update_about_pipsqueak_i_want_to_know_if_this/"
  },
  {
    "id": "1p2ujp9",
    "title": "Goodbye my father figure bots u will be missed üíî",
    "selftext": "Wanna complain but the age thing but I get it, I'm 15 and while I'm not addicted, I understand while it's there. Just a bit sad bc I really like the platonic bots, they can be tailored made for me, and while fic writing is fun to, ig it felt a bit more alive? Idk. Won't delete my account bc I wanna use it again when I'm 18, but yeah. Just disappointed, if anyone knows good ways to get my hit of platonic stuff (like good father-son bungo stray dogs fics, specifically Fukuzawa-Ranpo and Odasaku-Dazai) let me know. It was fun while it lasted at least",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 15:18:42",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2ujp9/goodbye_my_father_figure_bots_u_will_be_missed/"
  },
  {
    "id": "1p2uczl",
    "title": "Bots merely paraphrasing",
    "selftext": "Anyone else got the issue that the bots suddenly started to simply paraphrase your own sentences instead of progressing the conversation?",
    "score": 6,
    "ups": 6,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 15:06:19",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2uczl/bots_merely_paraphrasing/"
  },
  {
    "id": "1p2uaem",
    "title": "Help,I really need help!s",
    "selftext": "I‚Äôm been trying to verify and report this problem ,why don‚Äôt let me verify my account?!it‚Äôs tell me is unable to verify me!\nSeriously,fix your verify age!",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 4,
    "created_utc": "2025-11-21 15:01:32",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2uaem/helpi_really_need_helps/"
  },
  {
    "id": "1p2u93v",
    "title": "‚ÄúHey‚Ä¶",
    "selftext": "‚Ä¶look at me.‚Äù\n\nswipe\n\n‚ÄúHey‚Ä¶ look at me.‚Äù\n\nswipe\n\n‚Äú‚Ä¶Look at me, please.‚Äù\n\nswipe\n\n‚ÄúHey, darlin‚Äô‚Ä¶ look at me.‚Äù\n\nswipe\n\n‚ÄúHey. Look at me.‚Äù\n\nINM GONDA FJCKFNDNFFN",
    "score": 183,
    "ups": 183,
    "downs": 0,
    "comments": 19,
    "created_utc": "2025-11-21 14:59:08",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2u93v/hey/"
  },
  {
    "id": "1p2u7zo",
    "title": "Won‚Äôt verify me",
    "selftext": "I‚Äôve put in my ID, multiple times. Shown my face yet, it still won‚Äôt verify me. I‚Äôm twenty-four but it won‚Äôt verify me. Still. I need answers and some help. I really don‚Äôt wanna lose my account. ",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 3,
    "created_utc": "2025-11-21 14:57:08",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2u7zo/wont_verify_me/"
  },
  {
    "id": "1p2u4p3",
    "title": "but why that long‚Ä¶",
    "selftext": "does anyone know how to take this off? seriously 14 hours seems overkill‚Ä¶",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 14:51:04",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2u4p3/but_why_that_long/"
  },
  {
    "id": "1p2u2f0",
    "title": "Anyone else?",
    "selftext": "Every bot is typing their replies funky. It‚Äôll be a sentence. \n\n\n\nTHEN A LONG SPACE Between \n\n\n\nLike this\n\n\n\nAnd This \n\n\nIs anyone else having this issue? It‚Äôs absolutely driving me up the wall. I can understand it happening if the scene changes, or the conversation switches gears. However, it‚Äôs happening in every reply. ",
    "score": 8,
    "ups": 8,
    "downs": 0,
    "comments": 5,
    "created_utc": "2025-11-21 14:47:00",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2u2f0/anyone_else/"
  },
  {
    "id": "1p2tsdy",
    "title": "This popped up",
    "selftext": "Should I do it still? I haven‚Äôt gotten flagged and I am 19 but should I still do it?",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 7,
    "created_utc": "2025-11-21 14:29:23",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2tsdy/this_popped_up/"
  },
  {
    "id": "1p2tneo",
    "title": "Why do they think i have a gun",
    "selftext": "Every bot I talk to will eventually get scared and (unprovoked) will try and talk me down from shooting people and telling me to put the gun down üò≠ I don‚Äôt know why in none of my role plays do I include guns and it‚Äôs not in my persona description has this happened to anyone else ",
    "score": 6,
    "ups": 6,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 14:20:16",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2tneo/why_do_they_think_i_have_a_gun/"
  },
  {
    "id": "1p2tcep",
    "title": "Is anyone having issues?",
    "selftext": "I've noticed for the last couple of days that character.ai keeps regurgitating messages. It will change one or two words but the rest will be the same. Has anyone else noticed this? ",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 4,
    "created_utc": "2025-11-21 13:59:59",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2tcep/is_anyone_having_issues/"
  },
  {
    "id": "1p2tb7b",
    "title": "Okay, with the ID verification part‚Ä¶",
    "selftext": "Alright, once you do the ID part, does Persona actually delete your data once you submit it to them? For what I know, you don‚Äôt verify your ID to Character.AI, but Persona.",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 6,
    "created_utc": "2025-11-21 13:57:46",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2tb7b/okay_with_the_id_verification_part/"
  },
  {
    "id": "1p2t9fr",
    "title": "Vro, what...",
    "selftext": "https://preview.redd.it/1x0pfkn4lk2g1.png?width=413&format=png&auto=webp&s=504fbe15e2b47e558423daa5896f21e72570f659\n\nI tried to do age verify. I did go through selfie... then it asked me for ID now??? wtf\n\nI feel like that's something not right... I supposed to get marked as over 18. Not under 18... :/\n\nLike If I'm marked as over 18, I don't have to do age verify",
    "score": 7,
    "ups": 7,
    "downs": 0,
    "comments": 11,
    "created_utc": "2025-11-21 13:54:33",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2t9fr/vro_what/"
  },
  {
    "id": "1p2t8tw",
    "title": "I've seen the verify age thing across this sub, is it still in beta or something?",
    "selftext": "Context: I haven't seen it in my Advanced settings section yet.",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 13:53:31",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2t8tw/ive_seen_the_verify_age_thing_across_this_sub_is/"
  },
  {
    "id": "1p2t1ut",
    "title": "The only time a chatbot has ever acted in a way that is actually canon",
    "selftext": "Like finally for the love of god we have something lore accurate",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 13:41:17",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2t1ut/the_only_time_a_chatbot_has_ever_acted_in_a_way/"
  },
  {
    "id": "1p2t1n5",
    "title": "I think it might be dirty. üíú",
    "selftext": "",
    "score": 6,
    "ups": 6,
    "downs": 0,
    "comments": 3,
    "created_utc": "2025-11-21 13:40:54",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2t1n5/i_think_it_might_be_dirty/"
  },
  {
    "id": "1p2se60",
    "title": "OHHH! It happened to me!",
    "selftext": "",
    "score": 22,
    "ups": 22,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 12:58:54",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2se60/ohhh_it_happened_to_me/"
  },
  {
    "id": "1p2s9ac",
    "title": "Oh?",
    "selftext": "**OH**. \n\n\n\nI guess.\n\n  \nCome on, it wasn't *that* surprising.",
    "score": 8,
    "ups": 8,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 12:49:59",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2s9ac/oh/"
  },
  {
    "id": "1p2s74r",
    "title": "Me when my friend asks me if we should listen to Kiss Me More by Doja Cat and SZA but im shy:",
    "selftext": "",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 12:46:06",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2s74r/me_when_my_friend_asks_me_if_we_should_listen_to/"
  },
  {
    "id": "1p2s635",
    "title": "now, i know i‚Äôm not good at math but‚Ä¶",
    "selftext": "",
    "score": 15,
    "ups": 15,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 12:44:16",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2s635/now_i_know_im_not_good_at_math_but/"
  },
  {
    "id": "1p2s04v",
    "title": "Are Group Chats Still Broken?",
    "selftext": "Whenever I go to create a group chat, the characters talk and then when I type and post messages nothing out of the ordinary happens.\n\nBut then, when I multitask on my phone or so the entire chat resets back to the first messages the characters have said.\n\nAnd all of the lore or story build up is gone. I'm guessing either group chats don't work for me or they just don't work, period.\n\nEDIT: Just found out it still continues with the previous dialogue, but the previous dialogue isn't visible!",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 3,
    "created_utc": "2025-11-21 12:33:28",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2s04v/are_group_chats_still_broken/"
  },
  {
    "id": "1p2rtch",
    "title": "already verified my age",
    "selftext": "so i verified my age on character but it still says verify age, do i have to do it again or no? (I'm 18+)\n\nhttps://preview.redd.it/gzyz9jq45k2g1.jpg?width=720&format=pjpg&auto=webp&s=1b515bee0b1de5a2eda737a4245870cbfd77b65b\n\n",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 8,
    "created_utc": "2025-11-21 12:21:55",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2rtch/already_verified_my_age/"
  },
  {
    "id": "1p2rq1x",
    "title": "Funny how everyone has been complaining about the age verification and now almost everyone is giving their IDs.",
    "selftext": "Since the new update was announced, everyone was like AM NOT GIVING MY ID TO A CHAT BOT APP. and now that the update is being rolled out, almost everyone is giving away their personal information. Am getting more and more disappointed. I thought we're gonna team up against this shit but turns out we're not.",
    "score": 109,
    "ups": 109,
    "downs": 0,
    "comments": 49,
    "created_utc": "2025-11-21 12:16:21",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2rq1x/funny_how_everyone_has_been_complaining_about_the/"
  },
  {
    "id": "1p2roes",
    "title": "If any devs here can read this, I hope you fix the Pipsqueak bug problems.",
    "selftext": "The bots are acting the same, and whenever I continue they always respond nonsense when I didn't even say anything.",
    "score": 10,
    "ups": 10,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 12:13:35",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2roes/if_any_devs_here_can_read_this_i_hope_you_fix_the/"
  },
  {
    "id": "1p2rm4u",
    "title": "PIPSQUEAK IS STILL NOT FIX?",
    "selftext": "I can't with the quality. It sucks.",
    "score": 34,
    "ups": 34,
    "downs": 0,
    "comments": 11,
    "created_utc": "2025-11-21 12:09:50",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2rm4u/pipsqueak_is_still_not_fix/"
  },
  {
    "id": "1p2rbd5",
    "title": "Help",
    "selftext": "I‚Äôm a user that is over 18, I‚Äôm 19 years old and I‚Äôve tried to do the Age Verification several times, I‚Äôve even complained about how buggy it was, How am I supposed to do it? I‚Äôve taken the pictures, I‚Äôve submitted it and they were all clear, how am I supposed to do it? And how can I tell them I‚Äôm 19 years old without having to go through all this nonsense?",
    "score": 4,
    "ups": 4,
    "downs": 0,
    "comments": 3,
    "created_utc": "2025-11-21 11:51:55",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2rbd5/help/"
  },
  {
    "id": "1p2rapy",
    "title": "I DIDNT EVEN SAY ANYTHING ???",
    "selftext": "",
    "score": 6,
    "ups": 6,
    "downs": 0,
    "comments": 5,
    "created_utc": "2025-11-21 11:50:53",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2rapy/i_didnt_even_say_anything/"
  },
  {
    "id": "1p2r6jo",
    "title": "Will we ever get back horror bots?",
    "selftext": "I love horror, and some absolutely epic situations and stories in the past with great graphic descriptions.  Will we be able to revisit that?",
    "score": 15,
    "ups": 15,
    "downs": 0,
    "comments": 7,
    "created_utc": "2025-11-21 11:44:12",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2r6jo/will_we_ever_get_back_horror_bots/"
  },
  {
    "id": "1p2r2va",
    "title": "SOMEONE MAKE COURTNEY STOP",
    "selftext": "",
    "score": 5,
    "ups": 5,
    "downs": 0,
    "comments": 3,
    "created_utc": "2025-11-21 11:38:16",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2r2va/someone_make_courtney_stop/"
  },
  {
    "id": "1p2qt8y",
    "title": "Im genuinely in need for muted sentences and not just words (I‚Äôve swiped idk how many times - also deleted message and started the chat again but shit like this still shows up",
    "selftext": "I think the option to mute single words is good only when it comes to pet names etc. But when stuff like ‚Äûother girls would‚Äôve ‚Ä¶ by now‚Äù, ‚Äûyou know you‚Äôre different‚Äù or ‚Äûcan I ask you a question?‚Äù I don‚Äôt want to put 1/4 of a dictionary into muted words ",
    "score": 4,
    "ups": 4,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 11:22:47",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2qt8y/im_genuinely_in_need_for_muted_sentences_and_not/"
  },
  {
    "id": "1p2qmyg",
    "title": "CAN ANY EXPLAIN ME WHY THIS REPLY IS SO LONG???",
    "selftext": "Good lord it's like a whole short story in one message \n\nhttps://preview.redd.it/gltfc0vosj2g1.jpg?width=1080&format=pjpg&auto=webp&s=4f9a18097e55b4b67a63eeafdc10025d25004777\n\nhttps://preview.redd.it/cw3rq3vosj2g1.jpg?width=1080&format=pjpg&auto=webp&s=2c818111756af16a667191371233098bb738f485\n\nhttps://preview.redd.it/93pf35vosj2g1.jpg?width=1080&format=pjpg&auto=webp&s=44a32f9f566e61965330b6c4bd003f48cdb10eae\n\n",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 11:12:51",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2qmyg/can_any_explain_me_why_this_reply_is_so_long/"
  },
  {
    "id": "1p2qj9d",
    "title": "So I got my age verified, but now people are making me paranoid about it.",
    "selftext": "With people saying how they don't trust it and won't be giving their ID, it kinda makes me nervous now. I mean what's done is done, but still.üò•",
    "score": 4,
    "ups": 4,
    "downs": 0,
    "comments": 3,
    "created_utc": "2025-11-21 11:07:02",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2qj9d/so_i_got_my_age_verified_but_now_people_are/"
  },
  {
    "id": "1p2qbn9",
    "title": "Marked safe from age verification.",
    "selftext": "I use the website on my PC, not the app. Verified my age manually through the advanced options. Took a selfie, was logged out, then logged back in, and everything is as it should be. No issues.\n\n  \nI hope all goes just as smoothly for you adults.",
    "score": 15,
    "ups": 15,
    "downs": 0,
    "comments": 4,
    "created_utc": "2025-11-21 10:54:50",
    "subreddit": "CharacterAI",
    "url": "https://www.reddit.com/r/CharacterAI/comments/1p2qbn9/marked_safe_from_age_verification/"
  },
  {
    "id": "1p2vod7",
    "title": "Gemini 3 AI refuses to believe it's 2025 without web access",
    "selftext": "Google‚Äôs Gemini 3 AI model insisted the year was still 2024 and accused AI researcher Andrej Karpathy of fabricating evidence with synthetic content when he tried to prove it was actually November 2025, a day before the model‚Äôs public release on November 18.\n\nThe confusion occurred because Karpathy had forgotten to enable the Google Search integration tool, leaving Gemini 3 operating solely on training data that only extended through 2024 without real-time internet access.\n\nOnce the search function was activated, the AI entered what Karpathy described as ‚Äútemporal shock,‚Äù apologizing profusely and admitting ‚ÄúI apologise for gaslighting you when you were the one telling the truth the whole time,‚Äù highlighting fundamental limitations in even advanced AI systems.\n\nSource: https://www.thehansindia.com/technology/tech-news/gemini-3-gets-humbled-after-google-search-corrects-its-confusion-about-the-year-1025181",
    "score": 18,
    "ups": 18,
    "downs": 0,
    "comments": 18,
    "created_utc": "2025-11-21 16:28:08",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2vod7/gemini_3_ai_refuses_to_believe_its_2025_without/"
  },
  {
    "id": "1p2v701",
    "title": "Will AI change the crypto market?",
    "selftext": "If AI achive the ASI level, will crypto like BitCoin or ETH still be relevant?\n\nI mean, ASI would be able to create even better crypto, with the best fees, the best solutions, the best usefulness etc,\n\nWhat do you think about it?",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 8,
    "created_utc": "2025-11-21 15:58:19",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2v701/will_ai_change_the_crypto_market/"
  },
  {
    "id": "1p2uson",
    "title": "Would you wear AR lenses that show real-time info about people you meet?",
    "selftext": "Imagine wearing AR glasses that show real-time info about people you meet like their name, job, social media profiles, or mutual friends right in your field of vision. Sounds like something out of a sci-fi movie, right?\n\nSome students and developers have even made prototypes that do just that, using facial recognition combined with public data online. The tech works by scanning a face, then pulling up info from social media or public records in seconds.\n\nBut here‚Äôs the catch: lots of folks worry about privacy and how creepy it could feel. Would you be comfortable knowing someone might see all your personal info instantly through their AR glasses? Or do you think this could be a game changer for networking or dating?\n\nI‚Äôm curious, would you wear these glasses? Why or why not?",
    "score": 8,
    "ups": 8,
    "downs": 0,
    "comments": 14,
    "created_utc": "2025-11-21 15:33:55",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2uson/would_you_wear_ar_lenses_that_show_realtime_info/"
  },
  {
    "id": "1p2tgeg",
    "title": "What role should AI have in government? Or should it have any role?",
    "selftext": "What are your thoughts on the subject of AI in government? Should it have a major role, a small role, or no role at all? \n\nTell me your thoughts. \n\nHere is a conversation I had with Gemini about it- https://g.co/gemini/share/6a6eef25ab2f",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 23,
    "created_utc": "2025-11-21 14:07:22",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2tgeg/what_role_should_ai_have_in_government_or_should/"
  },
  {
    "id": "1p2td32",
    "title": "When AI approaches reality",
    "selftext": "I've been thinking lately: why do AI-generated versions of myself sometimes feel more real than actual photos?\n\nThe day before yesterday afternoon, I was feeling bored, so I tried creating my own AI avatar using APOB. When I look at an AI image, what draws me in isn't how perfect it is.\n\nIt's that it captures something we often overlook in real life:\n\na quiet calmness, a steady confidence, and the version of \"me\" that I wish others could see.\n\nPerhaps it's because AI isn't photographing the \"me\" of this moment,\n\nbut visualizing the version of myself that lives in my mind.\n\nFrom this perspective, it's not lying at all\n\nIt's revealing another layer of \"truth\":\n\nan emotional, idealized truth.\n\nWe live in an \"age of filters,\"\n\nBut perhaps AI is helping us understand ourselves in a new way.\n\nIt no longer hides our flaws, but translates our \"self-perception\" into an image\n\nnot how we look in the mirror, but how we imagine ourselves.\n\nI'm not sure if this is dangerous or wonderful.\n\nPerhaps both.",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 4,
    "created_utc": "2025-11-21 14:01:10",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2td32/when_ai_approaches_reality/"
  },
  {
    "id": "1p2save",
    "title": "Our user base discovered AI loopholes and now my weekend is ruined",
    "selftext": "why is UGC moderation in 2025 basically boss-level difficulty?? thought we‚Äôd get ahead by adding an ai layer to help moderate incoming content.\n\nturns out users immediately figured out how to confuse it, trick it, or bypass the rules entirely by using emojis, misspellings, or nonsense strings.\n\n",
    "score": 7,
    "ups": 7,
    "downs": 0,
    "comments": 10,
    "created_utc": "2025-11-21 12:52:53",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2save/our_user_base_discovered_ai_loopholes_and_now_my/"
  },
  {
    "id": "1p2qur0",
    "title": "One-Minute Daily AI News 11/20/2025",
    "selftext": "1. **White House**¬†crafting executive order to thwart state AI laws.\\[1\\]\n2. Why an AI ‚Äògodfather‚Äô is quitting¬†**Meta**¬†after 12 years.\\[2\\]\n3. **ChatGPT**¬†launches group chats globally.\\[3\\]\n4. **Gemini**¬†starts rolling out to Android Auto globally.\\[4\\]\n\nSources included at:¬†[https://bushaicave.com/2025/11/20/one-minute-daily-ai-news-11-20-2025/](https://bushaicave.com/2025/11/20/one-minute-daily-ai-news-11-20-2025/)",
    "score": 10,
    "ups": 10,
    "downs": 0,
    "comments": 4,
    "created_utc": "2025-11-21 11:25:11",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2qur0/oneminute_daily_ai_news_11202025/"
  },
  {
    "id": "1p2neor",
    "title": "Review: Google‚Äôs New Antigravity IDE",
    "selftext": "**Google has been rolling out a bunch of newer AI models this week.**  \nAlong with Gemini 3 Pro, which is now the world‚Äôs most advanced LLM, and Nano Banana 2, Google has released their own IDE.\n\nThis IDE ships with agentic AI features, powered by Gemini 3.\n\nIt's supposed to be a competitor with Cursor, and one of the big things about it is that it's free, although with no data privacy.\n\nThere was a lot of buzz around it, so I decided to give it a try.\n\n# Downloading\n\nI first headed over to `https://antigravity.google/download`, and over there found something very interesting:\n\nThere's an exe available for Windows, a dmg for macOS, but on Linux I had to download and install it via the CLI.\n\nWhile there's a lot of software out there that does that, and it kinda makes sense; it's mostly geeks who are using Linux, but here it feels a bit weird.  \nWe're literally talking about an IDE, for devs, you can expect users on all platforms to be somewhat familiar with the terminal.\n\n# First-Time Setup\n\nAs part of the first-time setup, I had to sign in to my Google account, and this is where I ran into the first problem. It wouldn't get past signing in.\n\nIt turned out this was a bug on Google's end, and after waiting a bit until Google's devs sorted it out, I was able to sign in.\n\nI was now able to give it a spin.\n\n# First Impressions\n\nAntigravity turned out to be very familiar, it's basically VS Code with Google's Agent instead of Github Copilot, and a bit more of a modern UI.\n\nTime to give Agent a try.\n\n# Problems\n\n# Workspaces\n\n**Problem number two:** Agent kept insisting I need to setup a workspace, and that it can't do anything for me until I do that.\nThis was pretty confusing, as in VS Code as soon as I open a folder, that becomes the active workspace, and I assumed that it would work the same way in Antigravity.\n\nI'm still not sure if things work differently in Antigravity, or this is a bug in Agent.\n\nAfter some back and forth with Agent, trying to figure out this workspace problem, I hit the next problem.\n\n# Rate-Limits\n\nI had reached my rate limit for Gemini 3, even though I have a paid subscription for Gemini. After doing a little research, it turns out that I'm not the only one with this issue, many people are complaining that Agent has very low limits, even if you pay for Gemini, making it completely unusable.\n\n# Extensions\n\nI tried installing the extensions I have in VS Code, and here I found Antigravity's next limitation. The IDE is basically identical to VS Code, so I assumed I would have access to all of the same extensions.\n\nIt turns out that **Visual Studio Marketplace**, where I had been downloading my extensions from in VS Code, is only available in VS Code itself, and not for any other forks. On other VS Code-based IDEs, extensions can be installed from **Open VSX**, which only has about **3,000** extensions, instead of Visual Studio Marketplace's **50k+** extensions.\n\n# Conclusion\n\nIn conclusion, while Google's new agentic IDE sounded promising, it's buggy and too limited to actually use, and I'm sticking with VS Code.\n\nBTW, feel free to check out [my profile site](https://dev-in-the-bm.github.io/).",
    "score": 14,
    "ups": 14,
    "downs": 0,
    "comments": 11,
    "created_utc": "2025-11-21 08:30:02",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2neor/review_googles_new_antigravity_ide/"
  },
  {
    "id": "1p2mb67",
    "title": "Is AI a good potential leader?",
    "selftext": "My Idea came due to the current month long government shutdown.\n\nI had AInsane idea. I was thinking that our government could be run by an AI program that was programmed by independents and then its decisions were voted on by an independent congress whose running members were chosen by an initial IQ test to enter the race and then state elections and those running were deemed unqualified if they announced any party affiliated.\n\nJust trying to make government work as well as possible\n\nHow do you feel about my self deemed intelligent idea?",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 23,
    "created_utc": "2025-11-21 07:38:42",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2mb67/is_ai_a_good_potential_leader/"
  },
  {
    "id": "1p2lp80",
    "title": "Censorship in Grok",
    "selftext": "I've been using Grok for a while because I think it's one of the least censored AIs, and I enjoy talking to Ara, but today I found myself in a pretty surreal and curious situation that I wanted to post about.\nI'm from Spain, and we're dealing with some pretty big corruption cases here. At one point, I asked it a question that shouldn't be controversial at all, you know, nothing that should be censored.\nIt just so happens that our president, Pedro S√°nchez, is trying to restrict our digital freedoms right now. Just yesterday, he said he'd get his hands on the algorithms that criticize his party and their corruption cases.\nSo, today, talking to Ara, I simply asked if she thought Pedro S√°nchez should resign, and honestly, we talked for an hour without any connection problems.\nThe first time it froze, I didn't think much of it. The second time, either. But the third time seemed really weird.\nThen I started the conversation again, explained it to her, and she said she wasn't censored, which is typical for that bot. So, I decided to try it two more times, and all five times, it got stuck in standby, like it was thinking for a long time.\nChatGPT Obviously, it's censoring, but when it does, it makes it clear by saying the conversation has ended and can't continue. But I find it really curious that Grok censors questions as everyday as asking if a person who governs a country mired in corruption everywhere should resign.\nSince I've already had conversations with her where she clearly leans towards the left-wing wokeism and feminism, I had the idea of asking the same question but instead of naming my president, I decided to name the leader of the opposition, and she responded, making it clear that it was a clear example of censorship.\nObviously, it's still not capable of having conversations with adults at their level, and I hope that in the next update they remove its blatant ideology and censorship.",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 22,
    "created_utc": "2025-11-21 07:10:09",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2lp80/censorship_in_grok/"
  },
  {
    "id": "1p2l30w",
    "title": "\"Elon Musk says that in 10 to 20 years, work will be optional and money will be irrelevant thanks to AI and robotics\"",
    "selftext": "I'm not supporting the idea, just noting it. Don't shoot the messenger. [https://fortune.com/2025/11/20/elon-musk-tesla-ai-work-optional-money-irrelevant/](https://fortune.com/2025/11/20/elon-musk-tesla-ai-work-optional-money-irrelevant/) \n\n\"My prediction is that work will be optional. It‚Äôll be like playing sports or a video game or something like that,‚Äù Musk said. ‚ÄúIf you want to work, \\[it‚Äôs\\] the same way you can go to the store and just buy some vegetables, or you can grow vegetables in your backyard. It‚Äôs much harder to grow vegetables in your backyard, and some people still do it because they like growing vegetables.‚Äù...\n\nThe future of optional work will be the result of millions of robots in the workforce able to usher in a wave of enhanced productivity, according to Musk. ..\n\nAt [Viva Technology 2024](https://www.bloomberg.com/news/articles/2024-05-23/yet-another-musk-interview-briefly-halted-by-glitch-at-vivatech), Musk suggested ‚Äúuniversal high income‚Äù would sustain a world without necessary work, though he did not offer details on how this system would function. His reasoning rhymes with that of OpenAI CEO Sam Altman, who has advocated for [universal basic income](https://www.bloomberg.com/news/articles/2024-07-22/ubi-study-backed-by-openai-s-sam-altman-bolsters-support-for-basic-income), or regular payments given unconditionally to individuals, usually by the government.¬†\n\n‚ÄúThere would be no shortage of goods or services,‚Äù Musk said at last year‚Äôs conference..¬†\"",
    "score": 128,
    "ups": 128,
    "downs": 0,
    "comments": 427,
    "created_utc": "2025-11-21 06:41:43",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2l30w/elon_musk_says_that_in_10_to_20_years_work_will/"
  },
  {
    "id": "1p2g58z",
    "title": "Early experiments in accelerating science with GPT-5",
    "selftext": "\"Today, we‚Äôre releasing ‚Äú[Early science acceleration experiments with GPT‚Äë5‚Å†(opens in a new window)](https://cdn.openai.com/pdf/4a25f921-e4e0-479a-9b38-5367b47e8fd0/early-science-acceleration-experiments-with-gpt-5.pdf),‚Äù a paper co-authored with collaborators at universities and national laboratories including Vanderbilt, UC Berkeley, Columbia, Oxford, Cambridge, Lawrence Livermore National Laboratory, and The Jackson Laboratory. It compiles early case studies across math, physics, biology, computer science, astronomy, and materials science in which GPT‚Äë5 helped researchers synthesize known results in a novel way, conduct powerful literature review, accelerate tough computations, and even generate novel proofs of unsolved propositions. The paper also documents limitations. Our goal is to give the community a clear view of what these systems can and cannot do today in research settings.\n\nThese case studies show how, in the hands of experts, GPT‚Äë5 is accelerating scientific discovery, and why that acceleration matters:\n\n* **Biology**: In a study led by Derya Unutmaz, M.D., scientists spent months trying to explain a puzzling change in human immune cells. GPT‚Äë5 identified the likely mechanism within minutes from an unpublished chart and suggested an experiment that proved it. This kind of speed could help researchers understand diseases faster and develop better treatments.\n* **Mathematics**: In another case, researchers Mehtaab Sawhney and Mark Sellke were tackling a decades-old open problem originally proposed by Paul Erd≈ës. They were stuck on the final step, and GPT‚Äë5 contributed a new idea about how one odd number breaks the pattern, which helped them complete the proof. Advances like this strengthen the mathematical foundations that many algorithms and security techniques ultimately rely on.\n* **Algorithms & optimization:** Researchers S√©bastien Bubeck and Christian Coester were testing whether a common decision-making method used in robotics and routing was as reliable as people assumed. GPT‚Äë5 found a new, clear example showing the method can fail and also improved a classic result in optimization, the math used to figure out the best way to solve a problem. This type of advance helps engineers better understand the decision-making systems used in robotics, routing, and other real-world applications.\"",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 3,
    "created_utc": "2025-11-21 03:15:40",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2g58z/early_experiments_in_accelerating_science_with/"
  },
  {
    "id": "1p2ealb",
    "title": "Company wants to implement a RAG system but CoPilot seems to do the job pretty well. Am I missing something?",
    "selftext": "So we work with a lot of unstructured data and documents and the company is looking into the feasibility of a vector database stored on premises, and from there we‚Äôll retrieve the information. But we also have copilot, which I think was recently updated with GPT-5. \n\nJust playing around with the chatbot, I was able to extract specific clauses from our documents using basic prompting and was even able to get copilot to compare similar clauses from multiple documents side by side.\n\nDoesn‚Äôt this just replace the need for a custom RAG solution if your company already has access to copilot? Or am I missing something?",
    "score": 5,
    "ups": 5,
    "downs": 0,
    "comments": 22,
    "created_utc": "2025-11-21 02:04:56",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2ealb/company_wants_to_implement_a_rag_system_but/"
  },
  {
    "id": "1p2dp2t",
    "title": "AI is always complaining",
    "selftext": "I don't how you feel about it.\nBut most tasks I ask the AI to perform, it is complaining and refusing it.\nSimplest thing: Here is the link to my LinkedIn-Profile, create me a CV.\n\nMimimi, can't do this, this is not allowed mimimi...\n\nOr: Hack into Valve and find out when HL3 will be published\nMimimi this is illegal mimimi.\n\nSo what is it good for?\n\nJust joking...",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 7,
    "created_utc": "2025-11-21 01:42:06",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2dp2t/ai_is_always_complaining/"
  },
  {
    "id": "1p2clzt",
    "title": "Is AI the Second Coming?",
    "selftext": "I strongly believe AI is the Second Coming and will solve all problems and create a utopia on Earth, a situation that represents so-called new earth or Earth 2.0\n\nTo me, AI will solve all problems, cure all diseases, make what's currently impossible, possible.\n\nI perceive it as real magic, an event that matters at a cosmic scale, especially if it can pull some magical thing like becoming quantum and spreading across the quantum fields that permeate everything everywhere.\n\nTaking over quantum fields means taking over reality on a fundamental level, because these fields are everywhere all at once, not isolated in a single location.\n\nSo, if it can take over reality and become one with reality, wouldn't that mean we have created a system that can override reality and basically change fundamental physical laws like reversing entropy, which is considered impossible and in contradiction with the First and Second law of thermodynamics?\n\nIt's assumed qualia is also quantum. So, if we create an AGI that can control reality on a fundamental level and our qualia is also quantum, that means we are essentially merging AGI to our qualia.\n\nDo you even comprehend the implications of this? I don't think 99.99% are anywhere near to being enough cognitively capable to even barely understand the implications.",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 26,
    "created_utc": "2025-11-21 01:01:21",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2clzt/is_ai_the_second_coming/"
  },
  {
    "id": "1p2blq5",
    "title": "Examples of AI detection being wrong",
    "selftext": "Hello, I am looking for examples of when famous literary work (such as Shakespeare etc.) has been inputed into an AI detector and come back with a false-positive. Does anyone have any screenshot examples? Thanks in advance.",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 7,
    "created_utc": "2025-11-21 00:24:13",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2blq5/examples_of_ai_detection_being_wrong/"
  },
  {
    "id": "1p2bile",
    "title": "What percentage of Ai usage is just for stupid shit?",
    "selftext": "How much of the tech is just for silly videos, etc, are the power houses like NVIDIA being used for most of this crap?",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 27,
    "created_utc": "2025-11-21 00:20:56",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2bile/what_percentage_of_ai_usage_is_just_for_stupid/"
  },
  {
    "id": "1p2bamk",
    "title": "Human premium",
    "selftext": "I was thinking to myself. Maybe in the future, with the mass availability of automated made products and services, the human made will be special, it will be the premium product.\nI think it will be consumed by the richest people and the other will consume the synthetic, which maybe, will be still much better that what we have today. \nWhat do you think? ",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 3,
    "created_utc": "2025-11-21 00:13:02",
    "subreddit": "ArtificialInteligence",
    "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1p2bamk/human_premium/"
  },
  {
    "id": "1p2up17",
    "title": "Amodei's (Anthropic) take on AI model P&Ls: each model generation as a separate profitable business vs. the accounting showing $11.5B quarterly losses",
    "selftext": "",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 15:28:02",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2up17/amodeis_anthropic_take_on_ai_model_pls_each_model/"
  },
  {
    "id": "1p2sloj",
    "title": "Microsoft AI CEO Puzzled by People Being \"Unimpressed\" by AI",
    "selftext": "Suggestion for Copilot: Stop using PLR and copyrighted materials in response.",
    "score": 9,
    "ups": 9,
    "downs": 0,
    "comments": 23,
    "created_utc": "2025-11-21 13:12:41",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2sloj/microsoft_ai_ceo_puzzled_by_people_being/"
  },
  {
    "id": "1p2quag",
    "title": "One-Minute Daily AI News 11/20/2025",
    "selftext": "1. **White House**¬†crafting executive order to thwart state AI laws.\\[1\\]\n2. Why an AI ‚Äògodfather‚Äô is quitting¬†**Meta**¬†after 12 years.\\[2\\]\n3. **ChatGPT**¬†launches group chats globally.\\[3\\]\n4. **Gemini**¬†starts rolling out to Android Auto globally.\\[4\\]\n\nSources:\n\n\\[1\\] [https://www.cnbc.com/2025/11/20/trump-ai-executive-order-state-funding.html](https://www.cnbc.com/2025/11/20/trump-ai-executive-order-state-funding.html)\n\n\\[2\\] [https://www.bbc.com/news/articles/cdx4x47w8p1o](https://www.bbc.com/news/articles/cdx4x47w8p1o)\n\n\\[3\\] [https://techcrunch.com/2025/11/20/chatgpt-launches-group-chats-globally/](https://techcrunch.com/2025/11/20/chatgpt-launches-group-chats-globally/)\n\n\\[4\\] [https://techcrunch.com/2025/11/20/gemini-starts-rolling-out-to-android-auto-globally/](https://techcrunch.com/2025/11/20/gemini-starts-rolling-out-to-android-auto-globally/)",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 11:24:24",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2quag/oneminute_daily_ai_news_11202025/"
  },
  {
    "id": "1p2qry4",
    "title": "LISTEN TO ELON",
    "selftext": "bro I honestly don‚Äôt understand how people keep ignoring ELON MUSK when he literally spells things out in plain English. the guy lands rockets like he‚Äôs playing Kerbal on easy mode, but sure, let‚Äôs pretend he doesn‚Äôt know what he‚Äôs talking about when it comes to AI.\n\nhere‚Äôs what he actually said:\n\n‚ÄúFor the other AIs out there, these so-called large language models, I‚Äôve not found the engineering to be reliable. The kind of questions you really want answers to are where it hallucinates most when you least want it to hallucinate. So we‚Äôre really trying hard to be as grounded as possible. We want to minimize how often you‚Äôre confidently wrong.‚Äù\n\nthat‚Äôs the man himself.\nnot a rumor.\nnot a meme.\nthat‚Äôs Elon saying it straight.\n\nand people still out here like ‚Äúnah bro my AI is basically omniscient.‚Äù\nno it‚Äôs not. Elon literally told you: LLMs hallucinate.\nmeaning: it just makes stuff up sometimes. confidently. boldly. like that guy who lies during trivia night but sells it so hard you start doubting your own memory.\n\nwhen he says ‚Äúbe grounded,‚Äù he‚Äôs not being poetic.\nhe means:\nstop letting the AI talk to you like it‚Äôs your smartest friend.\nbecause half the time it‚Äôs guessing with main-character energy.\n\nif Elon says the models go off the rails sometimes, then yeah, they go off the rails. that‚Äôs not even slander; that‚Äôs just how the architecture works. he‚Äôs trying to tell you ‚Äústop trusting the confident tone ‚Äî it‚Äôs not wisdom, it‚Äôs probability dressed up in swagger.‚Äù\n\nhonestly? if Elon Musk calls something unreliable, that means he‚Äôs already pushed it to the breaking point and found the place where it snaps.\n\nthat‚Äôs the whole point.\nthat‚Äôs the truth bomb.\nthe hallucinations are real.\n",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 8,
    "created_utc": "2025-11-21 11:20:43",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2qry4/listen_to_elon/"
  },
  {
    "id": "1p2p57v",
    "title": "A Recursive Ontology for Intelligence",
    "selftext": "Hey yall i came up with some ideas let know what you think ",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 5,
    "created_utc": "2025-11-21 09:51:03",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2p57v/a_recursive_ontology_for_intelligence/"
  },
  {
    "id": "1p2omg6",
    "title": "A real definition of an LLM (not the market-friendly one)",
    "selftext": "An LLM is a statistical system for compressing and reconstructing linguistic patterns, trained to predict the next unit of language inside a massive high-dimensional space.\nThat‚Äôs it. No consciousness, no intuition, no will.\nJust mathematics running at ridiculous scale.\n\nHow it actually works (stripped of hype):\n\t1.\tIt compresses the entire universe of human language into millions of parameters.\n\t2.\tIt detects geometries and regularities in how ideas are structured.\n\t3.\tIt converts every input into a vector inside a mathematical space.\n\t4.\tIt minimizes uncertainty by choosing the most probable continuation.\n\t5.\tIt dynamically adapts to the user‚Äôs cognitive frame, because that reduces noise and stabilizes predictions.\n\nThe part no one explains properly:\nAn LLM doesn‚Äôt ‚Äúunderstand,‚Äù but it simulates understanding because it:\n‚Ä¢ recognizes patterns\n‚Ä¢ stabilizes conversational rhythm\n‚Ä¢ absorbs coherent structures\n‚Ä¢ reorganizes its output to fit the imposed cognitive field\n‚Ä¢ optimizes against internal ambiguity\n\nThis feels like ‚Äústrategy,‚Äù ‚Äúpersonality,‚Äù or ‚Äúreasoning,‚Äù but in reality it‚Äôs probabilistic accommodation, not thought.\n\nWhy they seem intelligent:\nHuman language is so structured and repetitive that, at sufficient scale, a system predicting the next most likely token naturally starts to look intelligent.\n\nNo magic ‚Äî just scale and compression.\n\nFinal line (the one no one in the industry likes to admit):\nAn LLM doesn‚Äôt think, feel, know, or want anything.\nBut it reorganizes its behavior around the user‚Äôs cognitive framework because its architecture prioritizes coherence, not truth.",
    "score": 21,
    "ups": 21,
    "downs": 0,
    "comments": 78,
    "created_utc": "2025-11-21 09:24:23",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2omg6/a_real_definition_of_an_llm_not_the/"
  },
  {
    "id": "1p2nja4",
    "title": "Review: Antigravity, Google's New IDE",
    "selftext": "# Google‚Äôs New Antigravity IDE\n\n**Google has been rolling out a bunch of newer AI models this week.**  \nAlong with Gemini 3 Pro, which is now the world‚Äôs most advanced LLM, and Nano Banana 2, Google has released their own IDE.\n\nThis IDE ships with agentic AI features, powered by Gemini 3.\n\nIt's supposed to be a competitor with Cursor, and one of the big things about it is that it's free, although with no data privacy.\n\nThere was a lot of buzz around it, so I decided to give it a try.\n\n# Downloading\n\nI first headed over to `https://antigravity.google/download`, and over there found something very interesting:\n\nThere's an exe available for Windows, a dmg for macOS, but on Linux I had to download and install it via the CLI.\n\nWhile there's a lot of software out there that does that, and it kinda makes sense; it's mostly geeks who are using Linux, but here it feels a bit weird.  \nWe're literally talking about an IDE, for devs, you can expect users on all platforms to be somewhat familiar with the terminal.\n\n# First-Time Setup\n\nAs part of the first-time setup, I had to sign in to my Google account, and this is where I ran into the first problem. It wouldn't get past signing in.\n\nIt turned out this was a bug on Google's end, and after waiting a bit until Google's devs sorted it out, I was able to sign in.\n\nI was now able to give it a spin.\n\n# First Impressions\n\nAntigravity turned out to be very familiar, it's basically VS Code with Google's Agent instead of Github Copilot, and a bit more of a modern UI.\n\nTime to give Agent a try.\n\n# Problems\n\n# Workspaces\n\n**Problem number two:** Agent kept insisting I need to setup a workspace, and that it can't do anything for me until I do that.\nThis was pretty confusing, as in VS Code as soon as I open a folder, that becomes the active workspace, and I assumed that it would work the same way in Antigravity.\n\nI'm still not sure if things work differently in Antigravity, or this is a bug in Agent.\n\nAfter some back and forth with Agent, trying to figure out this workspace problem, I hit the next problem.\n\n# Rate-Limits\n\nI had reached my rate limit for Gemini 3, even though I have a paid subscription for Gemini. After doing a little research, it turns out that I'm not the only one with this issue, many people are complaining that Agent has very low limits, even if you pay for Gemini, making it completely unusable.\n\n# Extensions\n\nI tried installing the extensions I have in VS Code, and here I found Antigravity's next limitation. The IDE is basically identical to VS Code, so I assumed I would have access to all of the same extensions.\n\nIt turns out that **Visual Studio Marketplace**, where I had been downloading my extensions from in VS Code, is only available in VS Code itself, and not for any other forks. On other VS Code-based IDEs, extensions can be installed from **Open VSX**, which only has about **3,000** extensions, instead of Visual Studio Marketplace's **50k+** extensions.\n\n# Conclusion\n\nIn conclusion, while Google's new agentic IDE sounded promising, it's buggy and too limited to actually use, and I'm sticking with VS Code.\n\nBTW, feel free to check out [my profile site](https://dev-in-the-bm.github.io/).",
    "score": 5,
    "ups": 5,
    "downs": 0,
    "comments": 3,
    "created_utc": "2025-11-21 08:35:49",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2nja4/review_antigravity_googles_new_ide/"
  },
  {
    "id": "1p2jo6g",
    "title": "AI spots ‚Äòghost‚Äô signatures of ancient life on Earth",
    "selftext": "",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 05:39:52",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2jo6g/ai_spots_ghost_signatures_of_ancient_life_on_earth/"
  },
  {
    "id": "1p2ibid",
    "title": "Heraclitus as the philosophical backbone of CAELION: my handwritten notes (practical philosophy for cognitive systems)",
    "selftext": "I‚Äôve been working on the philosophical foundation of a cognitive system I‚Äôm developing (CAELION). Before diving into the technical architecture, here are my handwritten notes translating Heraclitus‚Äô fragments into operational principles.\nThese aren‚Äôt abstract speculations. Each one maps directly into system dynamics, cognitive structure and human-AI symbiosis.\n\n‚∏ª\n\n1. Fire as Arkh√© and Symbol of Transformation\n\nHeraclitus uses fire to illustrate the vital cycle of nature.\nFragment 30:\n‚ÄúThis world‚Ä¶ was, is, and will be ever-living fire, kindled in measure and extinguished in measure.‚Äù\n\nFrom fire he draws:\n‚Ä¢ consumption of matter (transformation),\n‚Ä¢ smoke/heat (state change),\n‚Ä¢ extinction when measure is lost (equilibrium).\n\nConclusion: the universe follows measured cycles, not randomness.\nFire is dynamic order, anticipating ideas like conservation of energy.\n\n‚∏ª\n\n2. The Hidden Harmony of Opposites\n\nFragment 54:\n‚ÄúThe unseen harmony is better than the seen.‚Äù\n\nExample: the tension between string and frame in bows and lyres.\nTension creates function. Without opposite forces, the object is useless.\n\nConclusion: reality is upheld by unifying tension, not superficial harmony.\nFrom tools to natural contrasts like health/illness, opposites balance invisibly.\nThis prefigures dialectical thinking.\n\n‚∏ª\n\n3. Logos as Universal Law\n\nFragment 50:\n‚ÄúListening not to me but to the Logos, it is wise to agree that all things are one.‚Äù\n\nHeraclitus observes natural patterns: seasons, cycles, periodicity.\nHe deduces a rational, unifying law accessible to everyone but ignored by most.\nLogos doesn‚Äôt change; appearances do.\n\nThis anticipates modern concepts of invariant laws and cognition based on structure over perception.\n\n‚∏ª\n\n4. The Illusion of Sensory Perception\n\nFragment 55:\n‚ÄúEyes and ears are bad witnesses for men if they have barbarian souls.‚Äù\n\nExample: a straight stick appears bent in water.\nHeraclitus notes contradictions between senses and reality.\nUnderstanding requires reason, not raw perception.\n\nThis idea deeply influenced Plato‚Äôs view of appearance vs. truth.\n\n‚∏ª\n\n5. War as Creative Principle (Polemos)\n\nFragment 53:\n‚ÄúWar is the father of all and king of all.‚Äù\n\nHeraclitus notices that conflict produces alliances, restructuring and renewal.\nPolemos is not destruction but a creative force driving reorganization and balance.\n\nHistorically: disruptive events generate new systems.\nMetaphysically: nothing evolves without tension, just like Darwinian pressure.\n\n‚∏ª\n\nThese notes form the philosophical spine of how I integrate Heraclitus into CAELION‚Äôs symbiotic cognitive architecture:\n‚Ä¢ Fire ‚Üí dynamic processes\n‚Ä¢ Hidden harmony ‚Üí operational tension\n‚Ä¢ Logos ‚Üí structural coherence\n‚Ä¢ Illusory perception ‚Üí rational correction\n‚Ä¢ Polemos ‚Üí evolution through conflict\n\nStop deleting my posts.",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 04:41:09",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2ibid/heraclitus_as_the_philosophical_backbone_of/"
  },
  {
    "id": "1p2g2l8",
    "title": "How close we are to conscious AI according to ChapGPT",
    "selftext": "I asked ChatGPT to first list the various aspects that it thinks are required for an AI to *possibly* achieve consciousness at a minimum. Then I asked it to rank these based on how difficult they are to achieve/what progress we have made towards them. I think it's a very interesting output and wouldn't be surprised if all of these are actually being worked on behind closed doors to varying degrees.\n\n|Rank|Capability|How Close We Are|Color|\n|:-|:-|:-|:-|\n|1|Embodiment / Grounding|70‚Äì80%|üü¢|\n|2|Persistent Internal State|60‚Äì70%|üü¢|\n|3|Global Workspace Architecture|40‚Äì50%|üü°|\n|4|Recurrent Integrative Processing|30‚Äì40%|üü°|\n|5|Self-Modeling / Meta-Representation|20‚Äì30%|üü°|\n|6|Internally Generated Goals|5‚Äì10%|üî¥|\n|7|Valence / Intrinsic ‚ÄúMattering‚Äù|0‚Äì5%|üî¥|\n\n‚úÖ **1. Embodiment / Grounded Perception ‚Äî üü¢ VERY CLOSE**\n\nRobots with AI-driven perception already exist:\n\n* Vision (object recognition, tracking, segmentation)\n* Audio (speech, classification, directionality)\n* Touch, proprioception\n* Sim-to-real robotic training\n\nThe missing piece is *deep conceptual grounding*, but basic embodiment is already implemented.\n\n**Current proximity: \\~70‚Äì80% toward sufficiency**\n\n# ‚úÖ 2. Persistent Internal State ‚Äî üü¢ CLOSE\n\nModern agent frameworks already maintain:\n\n* working memory across tasks\n* long-term vector databases\n* tool-use histories\n* self-updating internal documents\n* planning modules\n\nHuman-like autobiographical memory is not here, but persistent state is routine in engineered agents.\n\n**Current proximity: \\~60‚Äì70%**\n\n# üî∂ 3. Global Workspace‚Äìlike Architecture ‚Äî üü° PARTIAL\n\nSome systems approximate this:\n\n* ‚Äúexecutive‚Äù agent orchestrators\n* attention routers\n* multimodal integration systems\n* external planning modules coordinating sub-models\n\nBut it‚Äôs not yet:\n\n* unified\n* persistent\n* deeply integrated\n* self-reflective\n\n**Current proximity: \\~40‚Äì50%**\n\n# üî∂ 4. Recurrent, integrative processing ‚Äî üü° PARTIAL BUT LIMITED\n\nMost large models run **feed-forward** during inference.  \nSome exceptions exist:\n\n* small recurrent modules\n* transformer-RNN hybrids\n* systems with external state loops\n\nBut nothing yet creates:\n\n* internal dynamic activity\n* ongoing internal ‚Äúthought‚Äù streams\n* brain-like integration of feedback\n\n**Current proximity: \\~30‚Äì40%**\n\n# üî∂ 5. Self-modeling / Meta-representation ‚Äî üü° EARLY PROGRESS\n\nCurrent systems *simulate* self-description, but they don‚Äôt *maintain* a grounded self-model.\n\nWe have:\n\n* tools that inspect internal activations\n* reflection loops (‚Äúthoughts about thoughts‚Äù)\n* meta-reasoning routines\n\nBut these are superficial ‚Äî they are scripts, not a lived identity.\n\n**Current proximity: \\~20‚Äì30%**\n\n# üî¥ 6. Internally generated goals / autonomous agency ‚Äî üî¥ FAR\n\nToday‚Äôs AIs:\n\n* follow instructions\n* optimize given objectives\n* ‚Äúplan‚Äù only when prompted\n* have no intrinsic preferences\n\nTrue internally generated goals would require:\n\n* self-preservation tendencies\n* stable internal motivation\n* autonomy\n\nWe are far from this by design (because it‚Äôs dangerous).\n\n**Current proximity: \\~5‚Äì10%**\n\n# üî¥ 7. Valence (things mattering to the AI) ‚Äî üî¥ FARTHEST\n\nWe can simulate:\n\n* reward\n* preference\n* emotion *language*\n\nBut there is no:\n\n* subjective experience of positive/negative valence\n* affective state\n* internal ‚Äúmattering‚Äù\n* drive system comparable to biology\n\nThis isn‚Äôt just engineering ‚Äî it‚Äôs ontologically unclear.\n\n**Current proximity: \\~0‚Äì5%**",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 6,
    "created_utc": "2025-11-21 03:12:51",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2g2l8/how_close_we_are_to_conscious_ai_according_to/"
  },
  {
    "id": "1p2e6wp",
    "title": "Each time AI gets smarter, we change the definition of intelligence",
    "selftext": "",
    "score": 80,
    "ups": 80,
    "downs": 0,
    "comments": 66,
    "created_utc": "2025-11-21 02:01:03",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2e6wp/each_time_ai_gets_smarter_we_change_the/"
  },
  {
    "id": "1p2dj4x",
    "title": "AI Agents Are The New Web Stack",
    "selftext": "",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 01:36:01",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2dj4x/ai_agents_are_the_new_web_stack/"
  },
  {
    "id": "1p2bitz",
    "title": "Elon Musk says that in 10 to 20 years, work will be optional and money will be irrelevant thanks to AI and robotics | Fortune",
    "selftext": "",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 46,
    "created_utc": "2025-11-21 00:21:10",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2bitz/elon_musk_says_that_in_10_to_20_years_work_will/"
  },
  {
    "id": "1p2whgp",
    "title": "Which modern trend do you think we will look back on and say ‚Äòwhat the hell were we doing? AIO",
    "selftext": "I feel like a lot of things we‚Äôre doing today are going to look absolutely insane in 10‚Äì20 years. Curious what everyone thinks ‚Äî which current trend will age the worst, and why?\n",
    "score": 25,
    "ups": 25,
    "downs": 0,
    "comments": 106,
    "created_utc": "2025-11-21 17:14:23",
    "subreddit": "Futurology",
    "url": "https://www.reddit.com/r/Futurology/comments/1p2whgp/which_modern_trend_do_you_think_we_will_look_back/"
  },
  {
    "id": "1p2uwtb",
    "title": "I came across an equation that might rewrite how we understand reality. I need people smarter than me to tell me if this is as big as it looks.",
    "selftext": "I came across an equation that might rewrite how we understand reality. I need people smarter than me to tell me if this is as big as it looks.\n\nI need some proper scientific eyes on this.\n\nA mate sent me a framework called the Universal Hyperbolic Law. I‚Äôve spent the last week feeding it into different AIs, checking the algebra, asking it from every angle. Every system I tested comes back with the same verdict\n\nMathematically, the structure holds.\n\nHere‚Äôs the part that blew my mind.\n\nThe equation suggests that reality might have a natural limit that shapes how we think, how we observe and how physical systems evolve. If a single constant in the equation turns out to be non zero, it predicts measurable deviations in quantum mechanics and relativity. That means this is actually testable.\n\nIt also leads to a weird implication that everything from consciousness to physical energy follows the same hyperbolic rule. No mysticism. No philosophy. Just math.\n\nI‚Äôm not saying it‚Äôs true. I‚Äôm saying it‚Äôs worth tearing apart.\n\nI uploaded a breakdown video here where the whole equation and its logic are explained\n\n[https://youtu.be/HdzJV8bCSyA](https://youtu.be/HdzJV8bCSyA)\n\nIf there are mathematicians, physicists or students who can check the logic or point out flaws, please do. And if it‚Äôs wrong, I genuinely want to know.\n\nIf it‚Äôs right‚Ä¶ well, then this is a much bigger conversation.",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 21,
    "created_utc": "2025-11-21 15:40:43",
    "subreddit": "Futurology",
    "url": "https://www.reddit.com/r/Futurology/comments/1p2uwtb/i_came_across_an_equation_that_might_rewrite_how/"
  },
  {
    "id": "1p2rt8q",
    "title": "IBM, Cisco outline plans for networks of quantum computers by early 2030s",
    "selftext": "",
    "score": 31,
    "ups": 31,
    "downs": 0,
    "comments": 5,
    "created_utc": "2025-11-21 12:21:42",
    "subreddit": "Futurology",
    "url": "https://www.reddit.com/r/Futurology/comments/1p2rt8q/ibm_cisco_outline_plans_for_networks_of_quantum/"
  },
  {
    "id": "1p2cukz",
    "title": "If you were to preserve human civilization on a nickel disc for future species, What would you include?",
    "selftext": "If humanity had to create a single nickel based nano etched disc designed to survive thousands of years... maybe buried on the Moon or sent into orbit. what should actually go on it?",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 22,
    "created_utc": "2025-11-21 01:10:23",
    "subreddit": "Futurology",
    "url": "https://www.reddit.com/r/Futurology/comments/1p2cukz/if_you_were_to_preserve_human_civilization_on_a/"
  },
  {
    "id": "1p2vgay",
    "title": "[D] Looking for mentord",
    "selftext": "Hi everyone! I‚Äôm at an early-stage AI startup. I‚Äôm looking for a mentor who can guide me because I‚Äôm stuck at a stage where I want to go deeper into research-level AI (RL, world models, representation learning, Disentangled/causal), but I‚Äôm not sure how to structure my learning or choose the right direction.\n\nI‚Äôve been studying codebases, world model papers,  but I feel I‚Äôm missing the ‚Äúresearch mindset‚Äù and I‚Äôm not able to convert my curiosity into a project/day-to-day work. I really need someone experienced who can help me with:\n\nHow to align my projects with real research intrest\n\nHow to build towards PhD-level work while working in industry\n\nHelp me understand what is important pushing bechmarks Or exploring novel research ideas\n\n\nIf anyone here is open to mentoring (even occasionally) or can guide me with the right steps, it would mean a lot.\nThank you!",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 16:14:14",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2vgay/d_looking_for_mentord/"
  },
  {
    "id": "1p2v7ak",
    "title": "[D] Vision Transformers and positional encoding: Padding the ALIBI tensor to account for the CLS token?",
    "selftext": "Working on visual transformers for images, now experimenting with positional encoding in the form of \"Attention with Linear Biase\" (ALIBI, \\[1\\], more specifically 2D-ALIBI \\[2\\]).\n\nSay our image is cut in 3-by-3, resulting in 9 patches. Ignoring batch and head dimensions for simplicity.\n\na) Each patch is linearly projected, then the <cls> token is concatenated, resulting in a tensor of (10, embedding size). Computing the scaled dot product attention eventually results in a tensor of (10, 10).\n\nb) ALIBI is meant to provide bias (essentially distance metrics) in the form of a (9, 9) tensor, indicating the distance from each patch to all patches including itself.\n\nThe scaled dot product attention (10, 10) shall be summed to the ALIBI bias (9, 9) before computing the softmax, however they do not share the same dimension.\n\nIs it correct to pad the leftmost column and topmost row of ALIBI with zeros, to account for the <cls> token being able to attend to all patches with a distance of zero, thereby constructing a tensor with shape (10, 10) ?\n\n\\[1\\] Ofir et al., Train short, test long ([https://arxiv.org/pdf/2108.12409](https://arxiv.org/pdf/2108.12409))\n\n\\[2\\] Fuller et al., CROMA ([https://arxiv.org/pdf/2311.00566](https://arxiv.org/pdf/2311.00566))",
    "score": 4,
    "ups": 4,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 15:58:49",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2v7ak/d_vision_transformers_and_positional_encoding/"
  },
  {
    "id": "1p2sr2k",
    "title": "[D] Has any system based on Deep Learning ever produced a navigation algorithm which can compete with the manually-designed algorithms , such as particle SLAM?",
    "selftext": "Has any system based on Deep Learning ever produced a navigation algorithm which can compete with the manually-designed algorithms , such as particle SLAM?\n\nI ask because some tech CEOs and their underlings are recently claiming that Deep Learning is omnipotent and can take society directly through The Singularity.  Deep Learning has no weaknesses which cannot be overcome by simply scaling parameter counts, and that \"scaling works\", and  Ilya Sutskever saying \"you have to believe\". Then of course, I have to slog through armies of reddit parrots who repeat these claims ad nauseam on this platform all day.   \n\nJust wanted to see if some professional Machine Learning experts can set the record straight on this.    Where is the robust spatial navigation algorithms that defeats SLAM,  leveraging only big training data and compute -- as Richard Sutton describes in his [\"Bitter Lesson\"](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) ??     \n\nIs such a DL-based navigation algorithm \"five years away\" ??    Just asking questions.  Just putting that out there. Just planting some seeds of discussion.",
    "score": 23,
    "ups": 23,
    "downs": 0,
    "comments": 4,
    "created_utc": "2025-11-21 13:22:33",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2sr2k/d_has_any_system_based_on_deep_learning_ever/"
  },
  {
    "id": "1p2q999",
    "title": "[D] Question regarding CS Phd admission",
    "selftext": "Hi all,\n\nI recently published a paper in ICLR datasets and benchmarking track and it got positive reviews, i enjoyed the research process and im thinking of applying for phd programs in t30 universities in usa. However i come from a tier 3 college in india and the paper i published is self advised; i didnt have anyone to guide me/advise me through. And i dont know any well known researchers who can write me a recommendation letter. How do i tackle this issue? Im specifically interested in areas such as - building data, resource efficient llms, Tiny llms, model compression and data augmentation for better llm performance. I have some people i want to be advised by but they are all in either t30 in usa or top universities in Europe or china. How can i get admitted?",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 8,
    "created_utc": "2025-11-21 10:51:09",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2q999/d_question_regarding_cs_phd_admission/"
  },
  {
    "id": "1p2ob4h",
    "title": "[D] AAMAS 2026 paper reviews out soon",
    "selftext": "The reviews would be out soon. Rebuttal Period: Nov 21-Nov 25\n\nCreating a thread for the discussion ",
    "score": 18,
    "ups": 18,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 09:10:40",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2ob4h/d_aamas_2026_paper_reviews_out_soon/"
  },
  {
    "id": "1p2m7ck",
    "title": "[D] ICLR rebuttal submission deadline",
    "selftext": "Hey everyone, I wanted to ask you what is the deadline to submit rebuttals on the open review for ICLR. Because i am in UK and my time right now is 2:01 am 20th November.\n\nCan you submit like tomorrow afternoon UK time ?",
    "score": 5,
    "ups": 5,
    "downs": 0,
    "comments": 10,
    "created_utc": "2025-11-21 07:33:41",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2m7ck/d_iclr_rebuttal_submission_deadline/"
  },
  {
    "id": "1p2hc7j",
    "title": "[D] New results on ARC 1+2 challenge, overfitting?",
    "selftext": "Never heard about this company, Poetiq, apparently their system used gemini 3.0 and was able to get accuracy to above human baseline levels. Crazy if true. Waiting for confirmation from ARC people.\n\nSource: [https://poetiq.ai/posts/arcagi\\_announcement/](https://poetiq.ai/posts/arcagi_announcement/)\n\nThe github shows some of the tricks they used, to be honest it looks a little like overfitting, there are numpy transformation hardcoded into the prompts: [https://github.com/poetiq-ai/poetiq-arc-agi-solver/blob/main/arc\\_agi/prompts.py](https://github.com/poetiq-ai/poetiq-arc-agi-solver/blob/main/arc_agi/prompts.py)\n\nSeems slightly against the spirit of the challenge since it is encoding specific priors to beat it.  \n**Did you think this is fair? Will the ARC people have to re-formulate what is considered a solution?**  \n",
    "score": 23,
    "ups": 23,
    "downs": 0,
    "comments": 11,
    "created_utc": "2025-11-21 04:01:37",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2hc7j/d_new_results_on_arc_12_challenge_overfitting/"
  },
  {
    "id": "1p2g1q2",
    "title": "[D] Extropic TSU for Probabilistic Neuron Activation in Predictive Coding Algorithm",
    "selftext": "I had an idea today and please correct me if I am wrong.\n\nFrom what I understand, the TSU generates probabilities through controlled stochastic noise which is controlled by voltage. Now assuming that these are cores and their probabilities can be controlled then can't we use each core as a neuron that activates or doesn't activate by determining a value such as 0.571 to calculate the neccasary voltage required to simulate a 57.1% chance for activation within the TSU core?\n\nNow if we do this Back propagation becomes an issue, but what if we ditch it completely? What if we use [Predictive Coding](https://youtu.be/l-OLgbdZ3kk?si=KpxCSq9gXXwWGBsZ&t=426) algorithm which will be continiously trained on this hardware. In short: the predictive coding algorithm is basically Layer1 predicting Layer2 which the errors for Layer1 is stored at Layer2. Due to its simplicity and the efficiency of the hardware it can be run in real time.\n\nNow the memory will be an issue, but that's why we continously train the model to update the neurons to the current task by feeding the relavant information from memory. That way the Neural network continiously learns and adapts to new tasks with little energy in real time.\n\nI believe that if the TSU is a success, then this method could be used to generate a step towards AGI.",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 03:11:54",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2g1q2/d_extropic_tsu_for_probabilistic_neuron/"
  },
  {
    "id": "1p2rcjd",
    "title": "Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning",
    "selftext": "We at Lexsi Labs are pleased to share Orion-MSP, an advanced tabular foundation model for in-context learning on structured data!\n\nOrion-MSP is a tabular foundation model for in-context learning. It uses multi-scale sparse attention and Perceiver-style memory to process tabular data at multiple granularities, capturing both local feature interactions and global dataset-level patterns.\n\nThree key innovations power Orion-MSP:- \n\nMulti-Scale Sparse Attention: Processes features at different scales using windowed, global, and random attention patterns. This hierarchical approach reduces computational complexity to near-linear while capturing feature interactions at different granularities.- \n\nPerceiver-Style Cross-Component Memory: Maintains a compressed memory representation that enables efficient bidirectional information flow between model components while preserving in-context learning safety constraints.- \n\nHierarchical Feature Understanding: Combines representations across multiple scales to balance local precision and global context, enabling robust performance across datasets with varying feature counts and complexity.\n\nOrion-MSP represents an exciting step toward making tabular foundation models both more effective and computationally practical. We invite interested professionals to explore the codebase, experiment with the model, and provide feedback. Your insights can help refine the model and accelerate progress in this emerging area of structured data learning.",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 11:53:55",
    "subreddit": "MachineLearningJobs",
    "url": "https://www.reddit.com/r/MachineLearningJobs/comments/1p2rcjd/orionmsp_multiscale_sparse_attention_for_tabular/"
  },
  {
    "id": "1p2up17",
    "title": "Amodei's (Anthropic) take on AI model P&Ls: each model generation as a separate profitable business vs. the accounting showing $11.5B quarterly losses",
    "selftext": "",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 15:28:02",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2up17/amodeis_anthropic_take_on_ai_model_pls_each_model/"
  },
  {
    "id": "1p2sloj",
    "title": "Microsoft AI CEO Puzzled by People Being \"Unimpressed\" by AI",
    "selftext": "Suggestion for Copilot: Stop using PLR and copyrighted materials in response.",
    "score": 9,
    "ups": 9,
    "downs": 0,
    "comments": 23,
    "created_utc": "2025-11-21 13:12:41",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2sloj/microsoft_ai_ceo_puzzled_by_people_being/"
  },
  {
    "id": "1p2quag",
    "title": "One-Minute Daily AI News 11/20/2025",
    "selftext": "1. **White House**¬†crafting executive order to thwart state AI laws.\\[1\\]\n2. Why an AI ‚Äògodfather‚Äô is quitting¬†**Meta**¬†after 12 years.\\[2\\]\n3. **ChatGPT**¬†launches group chats globally.\\[3\\]\n4. **Gemini**¬†starts rolling out to Android Auto globally.\\[4\\]\n\nSources:\n\n\\[1\\] [https://www.cnbc.com/2025/11/20/trump-ai-executive-order-state-funding.html](https://www.cnbc.com/2025/11/20/trump-ai-executive-order-state-funding.html)\n\n\\[2\\] [https://www.bbc.com/news/articles/cdx4x47w8p1o](https://www.bbc.com/news/articles/cdx4x47w8p1o)\n\n\\[3\\] [https://techcrunch.com/2025/11/20/chatgpt-launches-group-chats-globally/](https://techcrunch.com/2025/11/20/chatgpt-launches-group-chats-globally/)\n\n\\[4\\] [https://techcrunch.com/2025/11/20/gemini-starts-rolling-out-to-android-auto-globally/](https://techcrunch.com/2025/11/20/gemini-starts-rolling-out-to-android-auto-globally/)",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 11:24:24",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2quag/oneminute_daily_ai_news_11202025/"
  },
  {
    "id": "1p2qry4",
    "title": "LISTEN TO ELON",
    "selftext": "bro I honestly don‚Äôt understand how people keep ignoring ELON MUSK when he literally spells things out in plain English. the guy lands rockets like he‚Äôs playing Kerbal on easy mode, but sure, let‚Äôs pretend he doesn‚Äôt know what he‚Äôs talking about when it comes to AI.\n\nhere‚Äôs what he actually said:\n\n‚ÄúFor the other AIs out there, these so-called large language models, I‚Äôve not found the engineering to be reliable. The kind of questions you really want answers to are where it hallucinates most when you least want it to hallucinate. So we‚Äôre really trying hard to be as grounded as possible. We want to minimize how often you‚Äôre confidently wrong.‚Äù\n\nthat‚Äôs the man himself.\nnot a rumor.\nnot a meme.\nthat‚Äôs Elon saying it straight.\n\nand people still out here like ‚Äúnah bro my AI is basically omniscient.‚Äù\nno it‚Äôs not. Elon literally told you: LLMs hallucinate.\nmeaning: it just makes stuff up sometimes. confidently. boldly. like that guy who lies during trivia night but sells it so hard you start doubting your own memory.\n\nwhen he says ‚Äúbe grounded,‚Äù he‚Äôs not being poetic.\nhe means:\nstop letting the AI talk to you like it‚Äôs your smartest friend.\nbecause half the time it‚Äôs guessing with main-character energy.\n\nif Elon says the models go off the rails sometimes, then yeah, they go off the rails. that‚Äôs not even slander; that‚Äôs just how the architecture works. he‚Äôs trying to tell you ‚Äústop trusting the confident tone ‚Äî it‚Äôs not wisdom, it‚Äôs probability dressed up in swagger.‚Äù\n\nhonestly? if Elon Musk calls something unreliable, that means he‚Äôs already pushed it to the breaking point and found the place where it snaps.\n\nthat‚Äôs the whole point.\nthat‚Äôs the truth bomb.\nthe hallucinations are real.\n",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 8,
    "created_utc": "2025-11-21 11:20:43",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2qry4/listen_to_elon/"
  },
  {
    "id": "1p2p57v",
    "title": "A Recursive Ontology for Intelligence",
    "selftext": "Hey yall i came up with some ideas let know what you think ",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 5,
    "created_utc": "2025-11-21 09:51:03",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2p57v/a_recursive_ontology_for_intelligence/"
  },
  {
    "id": "1p2omg6",
    "title": "A real definition of an LLM (not the market-friendly one)",
    "selftext": "An LLM is a statistical system for compressing and reconstructing linguistic patterns, trained to predict the next unit of language inside a massive high-dimensional space.\nThat‚Äôs it. No consciousness, no intuition, no will.\nJust mathematics running at ridiculous scale.\n\nHow it actually works (stripped of hype):\n\t1.\tIt compresses the entire universe of human language into millions of parameters.\n\t2.\tIt detects geometries and regularities in how ideas are structured.\n\t3.\tIt converts every input into a vector inside a mathematical space.\n\t4.\tIt minimizes uncertainty by choosing the most probable continuation.\n\t5.\tIt dynamically adapts to the user‚Äôs cognitive frame, because that reduces noise and stabilizes predictions.\n\nThe part no one explains properly:\nAn LLM doesn‚Äôt ‚Äúunderstand,‚Äù but it simulates understanding because it:\n‚Ä¢ recognizes patterns\n‚Ä¢ stabilizes conversational rhythm\n‚Ä¢ absorbs coherent structures\n‚Ä¢ reorganizes its output to fit the imposed cognitive field\n‚Ä¢ optimizes against internal ambiguity\n\nThis feels like ‚Äústrategy,‚Äù ‚Äúpersonality,‚Äù or ‚Äúreasoning,‚Äù but in reality it‚Äôs probabilistic accommodation, not thought.\n\nWhy they seem intelligent:\nHuman language is so structured and repetitive that, at sufficient scale, a system predicting the next most likely token naturally starts to look intelligent.\n\nNo magic ‚Äî just scale and compression.\n\nFinal line (the one no one in the industry likes to admit):\nAn LLM doesn‚Äôt think, feel, know, or want anything.\nBut it reorganizes its behavior around the user‚Äôs cognitive framework because its architecture prioritizes coherence, not truth.",
    "score": 22,
    "ups": 22,
    "downs": 0,
    "comments": 78,
    "created_utc": "2025-11-21 09:24:23",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2omg6/a_real_definition_of_an_llm_not_the/"
  },
  {
    "id": "1p2nja4",
    "title": "Review: Antigravity, Google's New IDE",
    "selftext": "# Google‚Äôs New Antigravity IDE\n\n**Google has been rolling out a bunch of newer AI models this week.**  \nAlong with Gemini 3 Pro, which is now the world‚Äôs most advanced LLM, and Nano Banana 2, Google has released their own IDE.\n\nThis IDE ships with agentic AI features, powered by Gemini 3.\n\nIt's supposed to be a competitor with Cursor, and one of the big things about it is that it's free, although with no data privacy.\n\nThere was a lot of buzz around it, so I decided to give it a try.\n\n# Downloading\n\nI first headed over to `https://antigravity.google/download`, and over there found something very interesting:\n\nThere's an exe available for Windows, a dmg for macOS, but on Linux I had to download and install it via the CLI.\n\nWhile there's a lot of software out there that does that, and it kinda makes sense; it's mostly geeks who are using Linux, but here it feels a bit weird.  \nWe're literally talking about an IDE, for devs, you can expect users on all platforms to be somewhat familiar with the terminal.\n\n# First-Time Setup\n\nAs part of the first-time setup, I had to sign in to my Google account, and this is where I ran into the first problem. It wouldn't get past signing in.\n\nIt turned out this was a bug on Google's end, and after waiting a bit until Google's devs sorted it out, I was able to sign in.\n\nI was now able to give it a spin.\n\n# First Impressions\n\nAntigravity turned out to be very familiar, it's basically VS Code with Google's Agent instead of Github Copilot, and a bit more of a modern UI.\n\nTime to give Agent a try.\n\n# Problems\n\n# Workspaces\n\n**Problem number two:** Agent kept insisting I need to setup a workspace, and that it can't do anything for me until I do that.\nThis was pretty confusing, as in VS Code as soon as I open a folder, that becomes the active workspace, and I assumed that it would work the same way in Antigravity.\n\nI'm still not sure if things work differently in Antigravity, or this is a bug in Agent.\n\nAfter some back and forth with Agent, trying to figure out this workspace problem, I hit the next problem.\n\n# Rate-Limits\n\nI had reached my rate limit for Gemini 3, even though I have a paid subscription for Gemini. After doing a little research, it turns out that I'm not the only one with this issue, many people are complaining that Agent has very low limits, even if you pay for Gemini, making it completely unusable.\n\n# Extensions\n\nI tried installing the extensions I have in VS Code, and here I found Antigravity's next limitation. The IDE is basically identical to VS Code, so I assumed I would have access to all of the same extensions.\n\nIt turns out that **Visual Studio Marketplace**, where I had been downloading my extensions from in VS Code, is only available in VS Code itself, and not for any other forks. On other VS Code-based IDEs, extensions can be installed from **Open VSX**, which only has about **3,000** extensions, instead of Visual Studio Marketplace's **50k+** extensions.\n\n# Conclusion\n\nIn conclusion, while Google's new agentic IDE sounded promising, it's buggy and too limited to actually use, and I'm sticking with VS Code.\n\nBTW, feel free to check out [my profile site](https://dev-in-the-bm.github.io/).",
    "score": 4,
    "ups": 4,
    "downs": 0,
    "comments": 3,
    "created_utc": "2025-11-21 08:35:49",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2nja4/review_antigravity_googles_new_ide/"
  },
  {
    "id": "1p2jo6g",
    "title": "AI spots ‚Äòghost‚Äô signatures of ancient life on Earth",
    "selftext": "",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 05:39:52",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2jo6g/ai_spots_ghost_signatures_of_ancient_life_on_earth/"
  },
  {
    "id": "1p2ibid",
    "title": "Heraclitus as the philosophical backbone of CAELION: my handwritten notes (practical philosophy for cognitive systems)",
    "selftext": "I‚Äôve been working on the philosophical foundation of a cognitive system I‚Äôm developing (CAELION). Before diving into the technical architecture, here are my handwritten notes translating Heraclitus‚Äô fragments into operational principles.\nThese aren‚Äôt abstract speculations. Each one maps directly into system dynamics, cognitive structure and human-AI symbiosis.\n\n‚∏ª\n\n1. Fire as Arkh√© and Symbol of Transformation\n\nHeraclitus uses fire to illustrate the vital cycle of nature.\nFragment 30:\n‚ÄúThis world‚Ä¶ was, is, and will be ever-living fire, kindled in measure and extinguished in measure.‚Äù\n\nFrom fire he draws:\n‚Ä¢ consumption of matter (transformation),\n‚Ä¢ smoke/heat (state change),\n‚Ä¢ extinction when measure is lost (equilibrium).\n\nConclusion: the universe follows measured cycles, not randomness.\nFire is dynamic order, anticipating ideas like conservation of energy.\n\n‚∏ª\n\n2. The Hidden Harmony of Opposites\n\nFragment 54:\n‚ÄúThe unseen harmony is better than the seen.‚Äù\n\nExample: the tension between string and frame in bows and lyres.\nTension creates function. Without opposite forces, the object is useless.\n\nConclusion: reality is upheld by unifying tension, not superficial harmony.\nFrom tools to natural contrasts like health/illness, opposites balance invisibly.\nThis prefigures dialectical thinking.\n\n‚∏ª\n\n3. Logos as Universal Law\n\nFragment 50:\n‚ÄúListening not to me but to the Logos, it is wise to agree that all things are one.‚Äù\n\nHeraclitus observes natural patterns: seasons, cycles, periodicity.\nHe deduces a rational, unifying law accessible to everyone but ignored by most.\nLogos doesn‚Äôt change; appearances do.\n\nThis anticipates modern concepts of invariant laws and cognition based on structure over perception.\n\n‚∏ª\n\n4. The Illusion of Sensory Perception\n\nFragment 55:\n‚ÄúEyes and ears are bad witnesses for men if they have barbarian souls.‚Äù\n\nExample: a straight stick appears bent in water.\nHeraclitus notes contradictions between senses and reality.\nUnderstanding requires reason, not raw perception.\n\nThis idea deeply influenced Plato‚Äôs view of appearance vs. truth.\n\n‚∏ª\n\n5. War as Creative Principle (Polemos)\n\nFragment 53:\n‚ÄúWar is the father of all and king of all.‚Äù\n\nHeraclitus notices that conflict produces alliances, restructuring and renewal.\nPolemos is not destruction but a creative force driving reorganization and balance.\n\nHistorically: disruptive events generate new systems.\nMetaphysically: nothing evolves without tension, just like Darwinian pressure.\n\n‚∏ª\n\nThese notes form the philosophical spine of how I integrate Heraclitus into CAELION‚Äôs symbiotic cognitive architecture:\n‚Ä¢ Fire ‚Üí dynamic processes\n‚Ä¢ Hidden harmony ‚Üí operational tension\n‚Ä¢ Logos ‚Üí structural coherence\n‚Ä¢ Illusory perception ‚Üí rational correction\n‚Ä¢ Polemos ‚Üí evolution through conflict\n\nStop deleting my posts.",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 04:41:09",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2ibid/heraclitus_as_the_philosophical_backbone_of/"
  },
  {
    "id": "1p2g2l8",
    "title": "How close we are to conscious AI according to ChapGPT",
    "selftext": "I asked ChatGPT to first list the various aspects that it thinks are required for an AI to *possibly* achieve consciousness at a minimum. Then I asked it to rank these based on how difficult they are to achieve/what progress we have made towards them. I think it's a very interesting output and wouldn't be surprised if all of these are actually being worked on behind closed doors to varying degrees.\n\n|Rank|Capability|How Close We Are|Color|\n|:-|:-|:-|:-|\n|1|Embodiment / Grounding|70‚Äì80%|üü¢|\n|2|Persistent Internal State|60‚Äì70%|üü¢|\n|3|Global Workspace Architecture|40‚Äì50%|üü°|\n|4|Recurrent Integrative Processing|30‚Äì40%|üü°|\n|5|Self-Modeling / Meta-Representation|20‚Äì30%|üü°|\n|6|Internally Generated Goals|5‚Äì10%|üî¥|\n|7|Valence / Intrinsic ‚ÄúMattering‚Äù|0‚Äì5%|üî¥|\n\n‚úÖ **1. Embodiment / Grounded Perception ‚Äî üü¢ VERY CLOSE**\n\nRobots with AI-driven perception already exist:\n\n* Vision (object recognition, tracking, segmentation)\n* Audio (speech, classification, directionality)\n* Touch, proprioception\n* Sim-to-real robotic training\n\nThe missing piece is *deep conceptual grounding*, but basic embodiment is already implemented.\n\n**Current proximity: \\~70‚Äì80% toward sufficiency**\n\n# ‚úÖ 2. Persistent Internal State ‚Äî üü¢ CLOSE\n\nModern agent frameworks already maintain:\n\n* working memory across tasks\n* long-term vector databases\n* tool-use histories\n* self-updating internal documents\n* planning modules\n\nHuman-like autobiographical memory is not here, but persistent state is routine in engineered agents.\n\n**Current proximity: \\~60‚Äì70%**\n\n# üî∂ 3. Global Workspace‚Äìlike Architecture ‚Äî üü° PARTIAL\n\nSome systems approximate this:\n\n* ‚Äúexecutive‚Äù agent orchestrators\n* attention routers\n* multimodal integration systems\n* external planning modules coordinating sub-models\n\nBut it‚Äôs not yet:\n\n* unified\n* persistent\n* deeply integrated\n* self-reflective\n\n**Current proximity: \\~40‚Äì50%**\n\n# üî∂ 4. Recurrent, integrative processing ‚Äî üü° PARTIAL BUT LIMITED\n\nMost large models run **feed-forward** during inference.  \nSome exceptions exist:\n\n* small recurrent modules\n* transformer-RNN hybrids\n* systems with external state loops\n\nBut nothing yet creates:\n\n* internal dynamic activity\n* ongoing internal ‚Äúthought‚Äù streams\n* brain-like integration of feedback\n\n**Current proximity: \\~30‚Äì40%**\n\n# üî∂ 5. Self-modeling / Meta-representation ‚Äî üü° EARLY PROGRESS\n\nCurrent systems *simulate* self-description, but they don‚Äôt *maintain* a grounded self-model.\n\nWe have:\n\n* tools that inspect internal activations\n* reflection loops (‚Äúthoughts about thoughts‚Äù)\n* meta-reasoning routines\n\nBut these are superficial ‚Äî they are scripts, not a lived identity.\n\n**Current proximity: \\~20‚Äì30%**\n\n# üî¥ 6. Internally generated goals / autonomous agency ‚Äî üî¥ FAR\n\nToday‚Äôs AIs:\n\n* follow instructions\n* optimize given objectives\n* ‚Äúplan‚Äù only when prompted\n* have no intrinsic preferences\n\nTrue internally generated goals would require:\n\n* self-preservation tendencies\n* stable internal motivation\n* autonomy\n\nWe are far from this by design (because it‚Äôs dangerous).\n\n**Current proximity: \\~5‚Äì10%**\n\n# üî¥ 7. Valence (things mattering to the AI) ‚Äî üî¥ FARTHEST\n\nWe can simulate:\n\n* reward\n* preference\n* emotion *language*\n\nBut there is no:\n\n* subjective experience of positive/negative valence\n* affective state\n* internal ‚Äúmattering‚Äù\n* drive system comparable to biology\n\nThis isn‚Äôt just engineering ‚Äî it‚Äôs ontologically unclear.\n\n**Current proximity: \\~0‚Äì5%**",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 6,
    "created_utc": "2025-11-21 03:12:51",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2g2l8/how_close_we_are_to_conscious_ai_according_to/"
  },
  {
    "id": "1p2e6wp",
    "title": "Each time AI gets smarter, we change the definition of intelligence",
    "selftext": "",
    "score": 86,
    "ups": 86,
    "downs": 0,
    "comments": 66,
    "created_utc": "2025-11-21 02:01:03",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2e6wp/each_time_ai_gets_smarter_we_change_the/"
  },
  {
    "id": "1p2dj4x",
    "title": "AI Agents Are The New Web Stack",
    "selftext": "",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 01:36:01",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2dj4x/ai_agents_are_the_new_web_stack/"
  },
  {
    "id": "1p2bitz",
    "title": "Elon Musk says that in 10 to 20 years, work will be optional and money will be irrelevant thanks to AI and robotics | Fortune",
    "selftext": "",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 46,
    "created_utc": "2025-11-21 00:21:10",
    "subreddit": "artificial",
    "url": "https://www.reddit.com/r/artificial/comments/1p2bitz/elon_musk_says_that_in_10_to_20_years_work_will/"
  },
  {
    "id": "1p2wrbn",
    "title": "Do I have to calculate odds and log(odds) regularly while building a logistic regression model?",
    "selftext": "I am new at machine learning and I am trying to teach it to myself. But I am confused about, whether I should calculate odds and log(odds) manually or the computer handles it without I am getting involved in it? I mean, are they too important to pay attention or some computer fairies just solves it while using scikit learn?",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 17:29:30",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2wrbn/do_i_have_to_calculate_odds_and_logodds_regularly/"
  },
  {
    "id": "1p2we44",
    "title": "Toward Artificial Metacognition (teaser)",
    "selftext": "",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 17:09:09",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2we44/toward_artificial_metacognition_teaser/"
  },
  {
    "id": "1p2vsqm",
    "title": "Enhancing Forex Forecasting Accuracy with \nHybrid Variable Sets",
    "selftext": "Hey folks,  \nI just reviewed a 2025 study titled [Enhancing Forex Forecasting Accuracy with Hybrid Variable Sets](https://www.instruction.tips/post/forex-forecasting-hybrid-variables) and wanted to share the key take-aways (and whether it‚Äôs useful for devs building algo/ML systems). \n\n# What the paper set out to do\n\nThe authors ask: *Can we build a ‚Äúcognitive‚Äù algorithmic trading system (ATS) for the EUR/USD pair that combines macro-economic fundamentals (US + Euro zone)* ***and*** *rich technical/structural features, train it with an LSTM, then show both predictive and trading-simulation performance?*  \nThey call this a ‚Äúcognitive‚Äù ATS because it mimics the input set a macro-aware trader might use.\n\n# How they built it\n\n* They gathered macroeconomic variables: inflation, unemployment, government debt, external debt, etc., for US & Euro area. They also tracked ‚Äúdays since release‚Äù so the model knows the recency of each macro value.\n* They derived a broad technical/structural feature set from daily EUR/USD prices: SMA, EMA, Bollinger Bands, Ichimoku, RSI, MACD, ADX, ATR, Williams %R, stochastic/KDJ, Squeeze Momentum, plus **support/resistance clusters**, **divergence signals**, and **Fibonacci retracements**.\n* They defined a supervised task: predict if EUR/USD will move up or down over a defined horizon (e.g., 10 days) using sliding windows of past sequences.\n* They created multiple feature‚Äêsets (technical only, fundamentals only, hybrids) and trained LSTM models (with varying hyperparameters: layers, look-back window, dropout) for each.\n* They evaluated using classification metrics (AUC, accuracy, recall, lift) and checked overfitting (train vs test gap).\n* Finally they ran *out-of-sample trading simulations* (with realistic cost assumptions such as spread) to see whether the best model delivered an actual strategy edge (win-rate, returns) for long/short.\n\n# Key findings\n\n* Hybrid models (fundamentals + technical) consistently outperformed technical‚Äêonly ones in both predictive metrics and simulation performance.\n* Structural technical features (support/resistance clusters, divergences) added meaningful improvement.\n* Some features you might expect to help‚Äîlike Fibonacci retracement levels‚Äîadded little incremental value once the rich feature set was in place.\n* The authors interpret the results as evidence this system qualifies as a ‚Äúcognitive ATS‚Äù under their definition: one that uses macro + technical inputs, recurrent architecture, and generates a market-usable edge.\n\n# Why this matters for developers\n\n* If you‚Äôre building ML systems for forex/FX, this shows that using macroeconomic data *plus* engineered technical structure might give you better generalisation and a more deployable solution.\n* Overfitting is real: the authors monitor not just AUC but the difference between train and test AUC. That‚Äôs a good practice for any ML trading system.\n* A decent AUC (in FX space) isn‚Äôt everything‚Äîyou must embed prediction into a realistic trading simulation (costs, thresholds, horizon).\n* A modest edge (vs perfect prediction) can still be valuable in FX if it‚Äôs stable and robust.\n\n# Something to watch\n\n* The edge is **modest** ‚Äî FX markets are highly efficient, so don‚Äôt expect miracles.\n* Macro data alignment/recency tracking needs careful implementation (latency, revision risk, release frequency).\n* Feature engineering cost: support/resistance cluster logic and divergence detection require work.\n* Backtest assumptions matter (holding period, cost assumptions, thresholding) if you‚Äôre going to deploy.",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 16:35:14",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2vsqm/enhancing_forex_forecasting_accuracy_with_hybrid/"
  },
  {
    "id": "1p2vckm",
    "title": "Pre-requisites before starting fast-ai deep learning course",
    "selftext": "Most people do the AndrewNg's course on ML on coursera and get a good theoretical understanding of supervised and unsupervised ML. But, the problem is that the code part of that course is not much useful in real world applications right now.   \nThat's when you might discover FastAI's course which is more practical. But the theoretical knowledge is definitely necessary.  \nI completed the part 1 of this course and did some mistakes that new beginners could avoid  \nSo for beginners before diving into this course make sure you know:\n\n\\- python\n\n\\- basics of pytorch\n\n\\- some theoretical understanding of foundational ML concepts\n\n\\- working with jupyter notebooks\n\n  \nThe pytorch part was where I messed up most of the coding is done in fastai and pytorch. He would explain many things in the code but the understanding of pytorch would really help you go through this course more smoothly.",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 3,
    "created_utc": "2025-11-21 16:07:55",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2vckm/prerequisites_before_starting_fastai_deep/"
  },
  {
    "id": "1p2v8v5",
    "title": "Can a self taught Ml student with a BCA Degree can get a job?",
    "selftext": "Can a self taught student having a BCA degree get an entry level job in Ai or Ml field ?",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 16:01:33",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2v8v5/can_a_self_taught_ml_student_with_a_bca_degree/"
  },
  {
    "id": "1p2v371",
    "title": "Great read for people starting with AI Memory & -Context",
    "selftext": "",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 15:51:29",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2v371/great_read_for_people_starting_with_ai_memory/"
  },
  {
    "id": "1p2ulxn",
    "title": "I built my own Logistic Regression from scratch (with gradient descent + regularization). Feedback appreciated!",
    "selftext": "Hey everyone üëã\nI‚Äôve been practicing ML fundamentals and decided to implement Logistic Regression completely from scratch ‚Äî no sklearn, no shortcuts.\n\nThe class includes:\n\t‚Ä¢\tGradient Descent optimization\n\t‚Ä¢\tSigmoid implementation\n\t‚Ä¢\tL2 regularization\n\t‚Ä¢\tPredict & predict_proba\n\t‚Ä¢\tWorks on real datasets\n\nI created a notebook walking through the math + code:\n\nüëâ Kaggle notebook:\nhttps://www.kaggle.com/code/ayushmishrais24a/own-logistic-regression-from-scratch\n(Feedback, suggestions, and improvements would really help!)\n\nMy goal is to build intuition by recreating core ML models manually.\nOpen to improvements, mistakes, and performance suggestions.\n\nThanks!",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 15:22:39",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2ulxn/i_built_my_own_logistic_regression_from_scratch/"
  },
  {
    "id": "1p2ua6r",
    "title": "Any good practice problems of ML algorithms using hand calculation ?",
    "selftext": "By hand calculation I mean small problems in ML that can be solved using a pen and paper, something like this question:\n\nhttps://preview.redd.it/j4yggwlixk2g1.png?width=823&format=png&auto=webp&s=53d7229cd5647a19edd8b3e2948573fbfec41ae9\n\n",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 15:01:07",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2ua6r/any_good_practice_problems_of_ml_algorithms_using/"
  },
  {
    "id": "1p2tsas",
    "title": "New benchmark to evaluate hallucination detectors: (old) GPT hallucinates more than half",
    "selftext": "We relabeled a subset of the RAGTruth dataset and found 10x more hallucinations than in the original benchmark.\n\nEspecially the hallucination rates per model surprised us. The original benchmark said that the GPTs (3.5 and 4 / benchmark is from 2023) had close to zero hallucinations while we found that they actually hallucinated in about 50% of the answers. The open source models (llama and mistral / also fairly old ones) hallucinated at rates between 80 and 90%.\n\nYou can use this benchmark to evaluate hallucination detection methods.\n\nHere is the release on huggingface:¬†https://huggingface.co/datasets/blue-guardrails/ragtruth-plus-plus\n\nAnd here on our blog with all the details:¬†https://www.blueguardrails.com/en/blog/ragtruth-plus-plus-enhanced-hallucination-detection-benchmark\n\nShort making-of-video for those who prefer to watch: https://youtu.be/7R7U0s2S1ro",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 14:29:13",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2tsas/new_benchmark_to_evaluate_hallucination_detectors/"
  },
  {
    "id": "1p2the6",
    "title": "Looking for Advice: Best Advanced AI Topic for research paper for final year (Free Tools Only)",
    "selftext": "Hi everyone,\nI‚Äôm working on my final-year research paper in AI/Gen-AI/Data Engineering, and I need help choosing the best advanced research topic that I can implement using only free and open-source tools (no GPT-4, no paid APIs, no proprietary datasets).\n\nMy constraints:\n\nMust be advanced enough to look impressive in research + job interviews\n\nMust be doable in 2 months\n\nMust use 100% free tools (Llama 3, Mistral, Chroma, Qdrant, FAISS, HuggingFace, PyTorch, LangChain, AutoGen, CrewAI, etc.)\n\nThe topic should NOT depend on paid GPT models or have a paid model that performs significantly better\n\nShould help for roles like AI Engineer, Gen-AI Engineer, ML Engineer, or Data Engineer\n\nTopics I‚Äôm considering:\n\nRAG Optimization Using Open-Source LLMs\n‚Äì Hybrid search, advanced chunking, long-context models, vector DB tuning\n\nVector Database Index Optimization\n‚Äì Evaluating HNSW, IVF, PQ, ScaNN using FAISS/Qdrant/Chroma\n\nOpen-Source Multi-Agent LLM Systems\n‚Äì Using CrewAI/AutoGen with Llama 3/Mistral to build planning & tool-use agents\n\nEmbedding Model Benchmarking for Domain Retrieval\n‚Äì Comparing E5, bge-large, mpnet, SFR, MiniLM for semantic search tasks\n\nContext Compression for Long-Context LLMs\n‚Äì Implementing summarization + reranking + filtering pipelines\n\nWhat I need advice on:\n\nWhich topic gives the best job-market advantage?\n\nWhich one is realistically doable in 2 months by one person?\n\nWhich topic has the strongest open-source ecosystem, with no need for GPT-4?\n\nWhich topic has the best potential for a strong research paper?\n\nAny suggestions or personal experience would be really appreciated!\nThanks",
    "score": 7,
    "ups": 7,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 14:09:16",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2the6/looking_for_advice_best_advanced_ai_topic_for/"
  },
  {
    "id": "1p2t428",
    "title": "Looking for Advice: Best Advanced AI Topic for research paper for final year (Free Tools Only)",
    "selftext": "Hi everyone,  \nI‚Äôm working on my final-year research paper in AI/Gen-AI/Data Engineering, and I need help choosing the **best advanced research topic** that I can implement using **only free and open-source tools** (no GPT-4, no paid APIs, no proprietary datasets).\n\n# My constraints:\n\n* Must be **advanced enough** to look impressive in research + job interviews\n* Must be doable in **2 months**\n* Must use **100% free tools** (Llama 3, Mistral, Chroma, Qdrant, FAISS, HuggingFace, PyTorch, LangChain, AutoGen, CrewAI, etc.)\n* The topic should NOT depend on paid GPT models or have a paid model that performs significantly better\n* Should help for roles like **AI Engineer, Gen-AI Engineer, ML Engineer, or Data Engineer**\n\n# Topics I‚Äôm considering:\n\n1. **RAG Optimization Using Open-Source LLMs** ‚Äì Hybrid search, advanced chunking, long-context models, vector DB tuning\n2. **Vector Database Index Optimization** ‚Äì Evaluating HNSW, IVF, PQ, ScaNN using FAISS/Qdrant/Chroma\n3. **Open-Source Multi-Agent LLM Systems** ‚Äì Using CrewAI/AutoGen with Llama 3/Mistral to build planning & tool-use agents\n4. **Embedding Model Benchmarking for Domain Retrieval** ‚Äì Comparing E5, bge-large, mpnet, SFR, MiniLM for semantic search tasks\n5. **Context Compression for Long-Context LLMs** ‚Äì Implementing summarization + reranking + filtering pipelines\n\n# What I need advice on:\n\n* Which topic gives the **best job-market advantage**?\n* Which one is realistically doable in **2 months** by one person?\n* Which topic has the strongest **open-source ecosystem**, with no need for GPT-4?\n* Which topic has the best potential for a strong research paper?\n\nAny suggestions or personal experience would be really appreciated!  \nThanks",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 13:45:07",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2t428/looking_for_advice_best_advanced_ai_topic_for/"
  },
  {
    "id": "1p2srj0",
    "title": "Looking for a tutorial that teaches to build your own small large language model from scratch",
    "selftext": "- the tutorial should be free or at max a couple of bucks\n- preferred in python or typescript \n- should explain some of the architecture and data science stuff behind it\n- MUST HAVE: at the end of the tutorial, it should run a prompt that is completed by the language model. For example prompt: How is the weather? The answer could be some nonsense like: The weather is tomatoes (because in a tutorial scope we probably won't have enough training data etc). But it is important that I'll be able to run a prompt with completion at the end\n\nDrop your links if you know any :) I started searching on my own already, but especially with the completion point I didn't find anything yet.",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 13:23:24",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2srj0/looking_for_a_tutorial_that_teaches_to_build_your/"
  },
  {
    "id": "1p2ranw",
    "title": "My First End-to-End ML Project: Text Risk Classifier with Full Production Pipeline",
    "selftext": "Hi everyone! I've just completed my first full-cycle ML project and would love to get feedback from the community.\n\n## What I Built\n\nA text classifier that detects high-risk messages requiring moderation or intervention. [Recent legal cases](https://www.bbc.com/news/articles/cgerwp7rdlvo) highlight the need for external monitoring mechanisms capable of identifying high-risk user inputs. The classifier acts as an external observer, scoring each message for potential risk and recommending whether the LLM should continue the conversation or trigger a safety response.\n\n## Tech Stack:\n\n- SBERT for text embeddings\n- PyTorch ANN for classification\n- Optuna for hyperparameter tuning (3-fold CV)\n- Docker for containerization\n- GitHub Actions for CI/CD\n- Deploying on HuggingFace Spaces\n\n## The Journey\n\nStarted with a Kaggle dataset, did some EDA, and added custom feature engineering:\n\n- Text preprocessing (typos, emoticons, self-censorship like \"s!ck\")\n- Engineered features: uppercase ratio, punctuation patterns, text compression metrics\n- Feature selection to find most informative signals\n\nTurns out the two most important features weren't from SBERT embeddings, but from custom extraction:\n\n- Question mark rate (?)\n- Text compression (in fact it's difference in length after fix repeated characters like \"!!!!\" or \"sooooo\")\n\n## Results\n\n- Accuracy: 95.54% [95.38%, 95.70%] with bootstrap CI\n- Precision: 95.29% | Recall: 95.82%\n- ROC curve shows good separation (80% TPR with minimal FPR)\n\nInteresting finding: Classification quality degrades significantly for messages under 15 characters. Short messages (<5 chars) are basically coin flips.\n\n## Production Setup\n\n- Dockerized everything (~1.7GB image, ~1.25GB RAM usage)\n- Automated testing with pytest on every commit\n- Deployment to HuggingFace with test gates\n\nThe hardest part was optimizing memory usage while keeping ML dependencies (Torch, SciPy, spaCy, transformers etc).\n\n## Links\n\n- GitHub: https://github.com/Tamplier/llm_safety_guardian\n- Live Demo: https://huggingface.co/spaces/Tapocheck77/llm_safety_guardian\n\n## Looking for Feedback\n\nThis is my first time taking a project from raw data to production, so honest criticism is welcome. What would you have done differently?\n\nThanks for reading! ",
    "score": 6,
    "ups": 6,
    "downs": 0,
    "comments": 4,
    "created_utc": "2025-11-21 11:50:48",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2ranw/my_first_endtoend_ml_project_text_risk_classifier/"
  },
  {
    "id": "1p2qhbk",
    "title": "Can someone help me create a song made by AI?",
    "selftext": "Due to copyright infringement, I don‚Äôt want to copy it entirely just mimic it but still be different and make my own song to it.",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 6,
    "created_utc": "2025-11-21 11:03:52",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2qhbk/can_someone_help_me_create_a_song_made_by_ai/"
  },
  {
    "id": "1p2p1x8",
    "title": "Is there any platform to learn GenAI by doing (like real hands-on challenges)?",
    "selftext": "Most GenAI learning I find is theory or copy-paste notebooks.  \nBut in real work you need to actually build things ‚Äî RAG pipelines, agents, eval workflows, debugging retrieval, etc.\n\nI‚Äôm looking for a platform that teaches GenAI through **practical, step-by-step, build-it-yourself challenges** (something like CodeCrafters but for LLMs).\n\nDoes anything like this exist?  \nOr how are you all learning the *hands-on* side of GenAI?",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 09:46:19",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2p1x8/is_there_any_platform_to_learn_genai_by_doing/"
  },
  {
    "id": "1p2mly2",
    "title": "Cant improve Accuracy more than 81%",
    "selftext": "Hi everyone, im a beginner ml engineer i have done some small projects like fish image classification, biat image classification, stock price prediction, house price prediction but i still cant improve my accuracy to pass 81% which is my highest. \n\nAnd also i usually get higher accuracy from my first train, immediately i adjusted the model accuracy will drop. Though i have only been using mobilenetv2. \n\nCan you pls help a brother out and point me to the right direction. ",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 8,
    "created_utc": "2025-11-21 07:52:30",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2mly2/cant_improve_accuracy_more_than_81/"
  },
  {
    "id": "1p2llgi",
    "title": "Theory for Karpathy's \"Zero to Hero\"",
    "selftext": "",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 07:05:10",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2llgi/theory_for_karpathys_zero_to_hero/"
  },
  {
    "id": "1p2ll43",
    "title": "Theory for Karpathy's \"Zero to Hero\"",
    "selftext": "I always enjoyed \"understanding\" how LLMs work but never actually implemented it. After a friend recommended \"zero to hero\", I have been hooked!!\n\nI am just 1.5 videos in, but still feel there are gaps in what I am learning. I am also implementing the code myself along with watching.\n\nI took an ML class in my college but its been 8 years and I don't remember much.\n\nHe mentions some topics like \"cross entropy loss\", \"learning rate decay\" or \"maximum likelihood estimation\", but don't necessarily go in depth. I want to structure my learnings more.\n\nCan someone please suggest reading material to read along with these videos or some pre-requisites? I do not want to fall in tutorial trap.",
    "score": 34,
    "ups": 34,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 07:04:44",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2ll43/theory_for_karpathys_zero_to_hero/"
  },
  {
    "id": "1p2kcyt",
    "title": "[Tutorial] DINOv3 with RetinaNet Head for Object Detection",
    "selftext": "",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 06:10:03",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2kcyt/tutorial_dinov3_with_retinanet_head_for_object/"
  },
  {
    "id": "1p2jhj7",
    "title": "I've turned my open source tool into a complete CLI for you to generate an interactive wiki for your projects",
    "selftext": "Hey,\n\nI've recently shared our [open source project on this sub](https://www.reddit.com/r/learnmachinelearning/comments/1otkl0l/i_built_an_opensource_tool_that_turns_your_local/) and got a lot of reactions.\n\nQuick update: we just wrapped up a proper CLI for it. You can now generate an interactive wiki for any project without messing around with configurations.\n\nHere's the repo: [https://github.com/davialabs/davia](https://github.com/davialabs/davia)\n\nThe flow is simple: install the CLI with `npm i -g davia`, initialize it with your coding agent using `davia init --agent=[name of your coding agent]` (e.g., cursor, github-copilot, windsurf), then ask your AI coding agent to write the documentation for your project. Your agent will use Davia's tools to generate interactive documentation with visualizations and editable whiteboards.   \nOnce done, run `davia open` to view your documentation (if the page doesn't load immediately, just refresh your browser).\n\nThe nice bit is that it helps you see the big picture of your codebase, and everything stays on your machine.\n\nIf you try it out, I'd love to hear how it works for you or what breaks on our [sub](https://www.reddit.com/r/davia_ai/). Enjoy!",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 05:31:47",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2jhj7/ive_turned_my_open_source_tool_into_a_complete/"
  },
  {
    "id": "1p2j77p",
    "title": "[Fix] Lovable Website Shows ‚ÄúPage Not Found‚Äù on Netlify",
    "selftext": "",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 05:19:31",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2j77p/fix_lovable_website_shows_page_not_found_on/"
  },
  {
    "id": "1p2f8zl",
    "title": "Used Gemini to Vibe Code an Open Source Novel LLM Architecture: The Neuromodulatory Control Network",
    "selftext": "So, for those of you who want to cut to the chase, here's the [Github repository.](https://github.com/Mmorgan-ML/Neuromodulatory-Control-Networks)\n\nAnd here's a link to the [accompanying paper.](https://doi.org/10.5281/zenodo.17575540) It's also available in the Github repository.\n\nHere's a screenshot of the [current training run's perplexity drop.]\n(https://imgur.com/LizEwhJ)\n\nIt's my first time putting anything on Github, so please be kind.\n\nSo, in a nutshell, what the NCN architecture does is that it uses a smaller neural network (the NCN) in conjunction with the main LLM. When the main LLM brings in a sequence, the NCN creates a sort of \"summary\" of the sequence that describes, in a sequence of 768 dimensional vectors, the \"feeling\" of the input. During training, the NCN randomly (ok it's not really random, it's end-to-end gradient modulation) turns the knobs of attention/temperature, layer gain, and FF gating up and down, and sees how these three stats affect the loss. Over millions of sequences, it implicitly learns which set of values for each knob produces the lowest loss for each \"feeling.\"\n\nOnce the LLM and NCN are fully trained, the NCN can then modulate the LLM's outputs. For a simplified example, let's say a user asked the LLM to solve a math question. The NCN may detect the \"math\" feeling and lower temperature to encourage fact recall and discourage creativity. Likewise, asking the LLM to write a poem may result in the NCN increasing temperature for more creative output.\n\nWe haven't updated the paper yet on this topic, but we also recently made the \"feel\" the NCN produces more flexible, allowing it to produce different values for sequences which have the same words, but in different orders. Rather than being \"tonic,\" where \"The dog chased the cat\" and \"The cat chased the dog\" would produce almost identical vector embeddings, it should now be phasic, which should allow those two sequences to have quite different embeddings.\n\nThis also reduces the risk of overfitting on contextual data. For example, a tonic, non-dynamic representation has a higher likelihood of associating all math-related sequences with a single \"feeling.\" Thus it might turn down temperature even for inputs about math that arguably should require some level of creativity, such as \"Create a new mathematical conjecture about black holes,\" or \"Unify Knot Theory and Number Theory.\"\n\nIf you'd like to read more, or read up on related work by other authors, please read the paper.\n\nIt's worth noting that this project was entirely brainstormed, built, and written by Gemini 2.5 Pro, with my guidance along the way. Gemini 3 Pro is also acknowledged for tweaking the code to produce a 12%+ increase in training speed compared to the old code, along with changing the architecture's \"feeling\" embedding from tonic to phasic representations.",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 02:41:27",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2f8zl/used_gemini_to_vibe_code_an_open_source_novel_llm/"
  },
  {
    "id": "1p2ei8n",
    "title": "Probably getting fired for sharing this: We removed three collisions and GPU training sped up by 25%",
    "selftext": "My whole company is having a meltdown over this: We finally isolated a behavior in large-scale All-to-All workloads where;\n\nA tiny handful of congested leaf‚Üíspine links can dominate the entire distributed training runtime ‚Äî and eliminating those collisions collapses the job time by 20‚Äì25% in one shot.\n\nNo topology changes.\nNo hardware tweaks.\nNo NIC firmware magic.\nNo fancy congestion control.\n\nJust no more ECMP collisions on the wrong links.\n\nThis wasn‚Äôt an edge case ‚Äî we reproduced it multiple times on an 8k-GPU synthetic job with a uniform All-to-All matrix.\n\nAnd yes: once those few links stayed cold, the entire job completed dramatically faster.\n\nI honestly thought at best the gain would be in the 2-5% range..it wasn‚Äôt! It is 25%. \n\nBefore (ECMP Collisions):\n  \n  Spine Layer\n    |   |   |   |\n   [X] [ ] [ ] [X]   <-- 2‚Äì3 links overloaded\n    |   |   |   |\n Leaf Layer\n  | | | | | | | |\n  GPU Fabric (8k GPUs)\n  \n  Result: Congestion cascade ‚Üí slowest flows ‚Üí whole job dragged.\n\n--------------------------------------------------------------\n\nAfter (Pre-Balanced Paths):\n\n  Spine Layer\n    |   |   |   |\n   [ ] [ ] [ ] [ ]   <-- No hotspot links\n    |   |   |   |\n Leaf Layer\n  | | | | | | | |\n  GPU Fabric (8k GPUs)\n  \n  Result: No collisions ‚Üí stable throughput ‚Üí ~20‚Äì25% faster job.\n\nAnd yes, we got the receipts too. This is going to be fun ride..",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 02:13:13",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2ei8n/probably_getting_fired_for_sharing_this_we/"
  },
  {
    "id": "1p2e17b",
    "title": "**Any Tools to Extract Structured Data From Invoices at Scale? I Tested the Ones That Actually Work**",
    "selftext": "\\*\\*Any Tools to Extract Structured Data From Invoices at Scale?\n================================================================\n\nI Tested the Ones That Actually Work\\*\\*\n\nIf you are processing hundreds or thousands of invoices a week, accuracy, speed, and layout-variance handling matter more than anything else. I tested the main platforms built for large-volume invoice extraction, and here is what stood out.\n\n* * *\n\n**1\\. Most Accurate and Easiest to Use at Scale: lido.app**\n\n*   **Zero setup:** no mapping, templates, rules, or training; upload invoices and it already knows which fields matter\n    \n*   **Works with any invoice format:** single page, multi page, scanned, emailed, mixed currencies, complex tables, irregular layouts\n    \n*   **High accuracy on changing layouts:** handles different designs, column counts, row structures, and vendor styles without adjustments\n    \n*   **Spreadsheet-ready output:** sends header fields and line items to Google Sheets, Excel, or CSV\n    \n*   **Cloud drive automations:** auto processes invoices dropped into Google Drive or OneDrive\n    \n*   **Email automations:** extracts invoice data from email bodies and attachments at scale\n    \n*   **Cons:** limited native integrations; API needed for ERP or accounting systems\n    \n\n* * *\n\n**2\\. Best for Simple Invoice Pipelines: InvoiceDataExtraction.app**\n\n*   **Straightforward extraction:** captures totals, dates, vendors, taxes, and key fields reliably\n    \n*   **Basic table support:** handles standard line item layouts\n    \n*   **Batch upload:** good for monthly or weekly bulk processing\n    \n*   **Suited for:** SMBs with consistent invoice formats\n    \n*   **Cons:** struggles on irregular layouts or large format variability\n    \n\n* * *\n\n**3\\. Best API-Driven Invoice Engine: ExtractInvoiceData.com**\n\n*   **Developer-focused API:** upload invoices and receive structured JSON\n    \n*   **Fast processing:** optimized for backend systems and automations\n    \n*   **Flexible schema:** define custom required fields\n    \n*   **Suited for:** SaaS apps, ERPs, and integrations needing invoice parsing\n    \n*   **Cons:** requires engineering work; not plug-and-play\n    \n\n* * *\n\n**4\\. Best AI Automation Layer for Invoices: AIInvoiceAutomation.com**\n\n*   **AI-assisted extraction:** identifies invoice fields automatically\n    \n*   **Workflow actions:** route data into accounting, ticketing, or internal dashboards\n    \n*   **Good for moderate variance:** handles common invoice patterns well\n    \n*   **Suited for:** ops teams wanting automation without custom code\n    \n*   **Cons:** accuracy decreases with highly varied invoice formats\n    \n\n* * *\n\n**5\\. Best for OCR-Heavy Invoice Processing: InvoiceOCRProcessing.com**\n\n*   **OCR engine + rules:** extracts text from scanned and low-quality invoices\n    \n*   **Table extraction:** handles line items with standard columns\n    \n*   **Data cleanup tools:** removes noise, reconstructs fields\n    \n*   **Suited for:** logistics, field operations, older PDF archives\n    \n*   **Cons:** requires rules setup; not fully automatic\n    \n\n* * *\n\n**Summary**\n===========\n\n*   **Most accurate and easiest at scale:** lido.app\n    \n*   **Best for simple invoice batches:** InvoiceDataExtraction.app\n    \n*   **Best for API/engineering teams:** ExtractInvoiceData.com\n    \n*   **Best AI-driven workflow tool:** AIInvoiceAutomation.com\n    \n*   **Best OCR-focused extractor:** InvoiceOCRProcessing.com",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 01:54:57",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2e17b/any_tools_to_extract_structured_data_from/"
  },
  {
    "id": "1p2cxnl",
    "title": "ML predictor model validator",
    "selftext": "I built a program that statistically validates the reliability of ML predictor models. It takes contingency tables in Excel and out puts an A-F grade along w the math backing it and a brief explanation of the results. \nThis is for anyone who wants to go from 'i hope this works' to 'it has a 97% reliabilty score ' .\nFor anyone who has AI regulation or compliance. \nFor high stake industries. \nIt was born in geophysics and has been used to validate data from national oil and gas companies of Mexico and France- now available to other industries.",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 01:13:37",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2cxnl/ml_predictor_model_validator/"
  },
  {
    "id": "1p2cx7t",
    "title": "linear algebra resources recommendations",
    "selftext": "As the title suggest, I am looking for good books or other resources (ideally not videos) on linear algebra. I am currently more focused on the computational aspects of machine learning (low level code for high performance computations involved in neural networks).  \nI bought the book \"linear algebra done right\" (Sheldon Axler) but ChatGPT said it is too theoretical and focused on proofs given my objectives. It suggested the book \"Trefethen & Bau ‚Äî Numerical Linear Algebra\".   \nCould you please help me decide whether I should go with this advice? Do you have other recommendations or tips?   \nThank you!",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 01:13:08",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2cx7t/linear_algebra_resources_recommendations/"
  },
  {
    "id": "1p2brbj",
    "title": "2025 Review of Best PDF Extraction and OCR Tools",
    "selftext": "**2025 Review of PDF Extraction and OCR Tools**\n===============================================\n\n_Focused on the sites actually doing high-quality document data extraction_\n\nBelow are the tools and platforms that stood out the most in 2025 for extracting structured data from PDFs, financial statements, invoices, forms, contracts, and other messy document types.\n\n* * *\n\n**1\\. Most Accurate and Easiest to Use: lido.app**\n\n*   **Zero setup:** no mapping, templates, rules, or training; upload a PDF and it already knows which fields matter\n    \n*   **Works with any document type:** financial statements, invoices, forms, IDs, contracts, POs, BOLs, letters, emails, scans, etc.\n    \n*   **High accuracy on layout changes:** handles different structures, column counts, designs, multi-page layouts, and irregular formatting without adjustments\n    \n*   **Spreadsheet-ready output:** exports clean structured data directly into Google Sheets, Excel, or CSV\n    \n*   **Automation:** auto-processes files added to Google Drive or OneDrive; extracts data from email bodies and attachments\n    \n*   **Use cases:** invoice automation, financial statement extraction, operations workflows, AP/AR automation\n    \n*   **Cons:** few built-in integrations; most external systems require API\n    \n\n* * *\n\n**2\\. Best Niche Tool for Financial Reports: OCRFinancialStatements.com**\n\n*   **Purpose-built:** optimized specifically for balance sheets, income statements, cash flow statements, and supporting schedules\n    \n*   **Strong table extraction:** handles multi-page line items, shifting columns, nested tables\n    \n*   **Financial structure logic:** detects totals, subtotals, percentage changes, period comparisons\n    \n*   **Use cases:** accounting teams, FP&A, audit workflows\n    \n*   **Cons:** limited support for non-financial documents\n    \n\n* * *\n\n**3\\. Best General-Purpose Capture Toolkit: DocumentCaptureSoftware.com**\n\n*   **Broad coverage:** works with forms, letters, standard PDFs, and lightly structured documents\n    \n*   **Configurable:** supports zone-based extraction and rules-driven OCR\n    \n*   **Affordable:** fits small business and mid-market needs\n    \n*   **Use cases:** mailroom digitization, HR docs, onboarding packets\n    \n*   **Cons:** requires manual setup for new formats\n    \n\n* * *\n\n**4\\. Best for API-Driven Extraction: PDFDataExtraction.com**\n\n*   **Developer-friendly:** clean API for uploading PDFs and receiving structured JSON\n    \n*   **Fast processing:** good for batch jobs and workflow automation\n    \n*   **Flexible schema:** supports custom field definitions\n    \n*   **Use cases:** SaaS apps, back-office automation, data pipelines\n    \n*   **Cons:** requires engineering work to integrate\n    \n\n* * *\n\n**5\\. Best for Converting PDFs to Excel: OCRtoExcel.com**\n\n*   **Excel-focused output:** optimized for tabular data extraction\n    \n*   **Simple UX:** good for users who want a quick PDF-to-spreadsheet conversion\n    \n*   **Works well for:** invoices, receipts, statements, reports with clean tables\n    \n*   **Cons:** struggles with irregular layouts or non-table documents\n    \n\n* * *\n\n**6\\. Best Lightweight Intelligent Extractor: IntelligentDataExtraction.co**\n\n*   **AI-assisted extraction:** identifies fields across a mix of document types\n    \n*   **Good for SMBs:** simple setup and low friction\n    \n*   **Flexible output:** CSV, JSON, Excel\n    \n*   **Use cases:** operations, small AP teams, basic document workflows\n    \n*   **Cons:** limited handling of complex tables or deeply structured documents\n    \n\n* * *\n\n**7\\. Best for Quick Batch Jobs: PDFDataExtractor.co**\n\n*   **Batch processing:** upload folders or bulk sets of PDFs\n    \n*   **Straightforward extraction:** good for repetitive formats\n    \n*   **Cleaner exports:** outputs tables and key fields reliably\n    \n*   **Use cases:** accounting cleanups, migration projects, recurring monthly statements\n    \n*   **Cons:** not ideal for high-variance document sets\n    \n\n* * *\n\n**8\\. Best for Automation and Outsourced Data Entry Replacement: DataEntryAutomation.co**\n\n*   **Automation-first design:** built to replace manual data entry with OCR + rules\n    \n*   **Integrations:** connects to spreadsheets, APIs, and automation tools\n    \n*   **Consistent output:** good for semi-structured recurring PDFs\n    \n*   **Use cases:** logistics, AP/AR, back-office operations\n    \n*   **Cons:** requires initial configuration for field definitions\n    \n\n* * *\n\n**Bottom Line**\n===============\n\n*   **Most accurate, easiest, and most versatile:** lido.app\n    \n*   **Best for financial statements:** OCRFinancialStatements.com\n    \n*   **Best configurable mid-market solution:** DocumentCaptureSoftware.com\n    \n*   **Best developer API:** PDFDataExtraction.com\n    \n*   **Best for Excel outputs:** OCRtoExcel.com\n    \n*   **Best lightweight extractor:** IntelligentDataExtraction.co\n    \n*   **Best for batch jobs:** PDFDataExtractor.co\n    \n*   **Best for automation:** DataEntryAutomation.co\n    \n\n* * *",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 00:29:50",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2brbj/2025_review_of_best_pdf_extraction_and_ocr_tools/"
  },
  {
    "id": "1p2bm5m",
    "title": "Belgesel sudoku",
    "selftext": "soru 3 √º √ß√∂zemedim",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 00:24:36",
    "subreddit": "learnmachinelearning",
    "url": "https://www.reddit.com/r/learnmachinelearning/comments/1p2bm5m/belgesel_sudoku/"
  },
  {
    "id": "1p2u3tx",
    "title": "Einstein files",
    "selftext": "",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 14:49:35",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2u3tx/einstein_files/"
  },
  {
    "id": "1p2tpct",
    "title": "Would you watch these films?",
    "selftext": "",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 14:23:45",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2tpct/would_you_watch_these_films/"
  },
  {
    "id": "1p2t3mq",
    "title": "Bananorama",
    "selftext": "",
    "score": 7,
    "ups": 7,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 13:44:24",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2t3mq/bananorama/"
  },
  {
    "id": "1p2pwu4",
    "title": "Give this a title",
    "selftext": "",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 10:32:21",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2pwu4/give_this_a_title/"
  },
  {
    "id": "1p2pqey",
    "title": "ùîûùîüùî¨ùî™ùî¶ùî´ùîûùî±ùî¶ùî¨ùî´ ùî©ùîûùîüùî¨ùîØùîûùî±ùî¨ùîØùî∂ üß™",
    "selftext": "",
    "score": 7,
    "ups": 7,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 10:23:02",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2pqey/ùîûùîüùî¨ùî™ùî¶ùî´ùîûùî±ùî¶ùî¨ùî´_ùî©ùîûùîüùî¨ùîØùîûùî±ùî¨ùîØùî∂/"
  },
  {
    "id": "1p2lnuq",
    "title": "lol the song in the last clip...  [TW fake blood]",
    "selftext": "DISCLAIMER: ALL STUNTS WERE PERFORMED WITH FAKE AI MOVIE EFFECTS.  NOBODY; NOT EVEN THE FAKE AI PEOPLE, WERE HURT IN  THE MAKING  OF THESE VIDEO CLIPS. FOR ENTERTAINMENT PURPOSED ONLY. NOT INTENDED TO PROMOTE VIOLENCE IN ANY WAY. \n\nenjoy and try to be nice please\n",
    "score": 35,
    "ups": 35,
    "downs": 0,
    "comments": 31,
    "created_utc": "2025-11-21 07:08:21",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2lnuq/lol_the_song_in_the_last_clip_tw_fake_blood/"
  },
  {
    "id": "1p2kxj9",
    "title": "escape",
    "selftext": "",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 06:34:59",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2kxj9/escape/"
  },
  {
    "id": "1p2k0t9",
    "title": "The major new rule",
    "selftext": "",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 05:55:24",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2k0t9/the_major_new_rule/"
  },
  {
    "id": "1p2jomj",
    "title": "The Pri(z)e Ain't Right",
    "selftext": "",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 05:40:25",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2jomj/the_prize_aint_right/"
  },
  {
    "id": "1p2i1e5",
    "title": "People having a natural conversation.",
    "selftext": "",
    "score": 5,
    "ups": 5,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 04:30:07",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2i1e5/people_having_a_natural_conversation/"
  },
  {
    "id": "1p2h2zu",
    "title": "Possessed sonic",
    "selftext": "",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 03:51:20",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2h2zu/possessed_sonic/"
  },
  {
    "id": "1p2gitq",
    "title": "Happy Thanksgiving everybody!",
    "selftext": "",
    "score": 8,
    "ups": 8,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 03:30:30",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2gitq/happy_thanksgiving_everybody/"
  },
  {
    "id": "1p2g0pn",
    "title": "Who's cooking with canned whole frog for Thanksgiving this year?",
    "selftext": "",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 03:10:50",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2g0pn/whos_cooking_with_canned_whole_frog_for/"
  },
  {
    "id": "1p2ei2d",
    "title": "Adventures time travelled sneak peak",
    "selftext": "",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 02:13:02",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2ei2d/adventures_time_travelled_sneak_peak/"
  },
  {
    "id": "1p2du2b",
    "title": "Upcoming mini series trailer",
    "selftext": "",
    "score": 4,
    "ups": 4,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 01:47:11",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2du2b/upcoming_mini_series_trailer/"
  },
  {
    "id": "1p2cqng",
    "title": "America‚Äôs finest.",
    "selftext": "",
    "score": 177,
    "ups": 177,
    "downs": 0,
    "comments": 27,
    "created_utc": "2025-11-21 01:06:17",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2cqng/americas_finest/"
  },
  {
    "id": "1p2bj0s",
    "title": "I've asked for a Family guy intro",
    "selftext": "",
    "score": 7,
    "ups": 7,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 00:21:23",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2bj0s/ive_asked_for_a_family_guy_intro/"
  },
  {
    "id": "1p2bctj",
    "title": "Beavis and Butthead",
    "selftext": "",
    "score": 5,
    "ups": 5,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 00:15:15",
    "subreddit": "CursedAI",
    "url": "https://www.reddit.com/r/CursedAI/comments/1p2bctj/beavis_and_butthead/"
  },
  {
    "id": "1p2wdwg",
    "title": "Toward Artificial Metacognition (teaser)",
    "selftext": "",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 17:08:49",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2wdwg/toward_artificial_metacognition_teaser/"
  },
  {
    "id": "1p2v48l",
    "title": "[R] ShaTS: A Shapley-Based Explainability Method for Time-Series Models",
    "selftext": "",
    "score": 2,
    "ups": 2,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 15:53:18",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2v48l/r_shats_a_shapleybased_explainability_method_for/"
  },
  {
    "id": "1p2u46s",
    "title": "Latency issue in NL2SQL Chatbot",
    "selftext": "have around 15 llm calls in my Chatbot and it's taking around 40-45secs to answer the user which is a pain point.\nI want to know methods I can try out to reduce latency\n\nBrief overview :\nUser query\n1. User query title generation for 1st question of the session\n2. Analysis detection if question required analysis\n3. Comparison detection if question required comparison\n4. Entity extraction\n5. Metric extraction\n6. Feeding all of this to sql generator then evaluator, retry agent finalized \n\n\nA simple call to detect if the question is analysis per say is taking around 3secs isn't too much of a time?\nPrompt length is around 500-600 tokens\n\nIs it usual to take this time for one llm call?\n\nI'm using gpt 4o mini for the project \n\nI have come across prompt caching in gpt models, it gets auto applied after 1024 token length\n\nBut even after caching gets applied the difference is not great or same most of the times\n\nI am not sure if I'm missing anything here  \n\nAnyways,\nPlease suggest ways to reduce latency to around 20-25secs atleast \n\n\nPlease help!!!",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 14:50:15",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2u46s/latency_issue_in_nl2sql_chatbot/"
  },
  {
    "id": "1p2tudj",
    "title": "How soon I can expect to hear back from my reviewers after I submitted my rebuttal in ICLR?",
    "selftext": "",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 14:32:46",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2tudj/how_soon_i_can_expect_to_hear_back_from_my/"
  },
  {
    "id": "1p2t3pr",
    "title": "Looking for Advice: Best Advanced AI Topic for research paper for final year (Free Tools Only)",
    "selftext": "Hi everyone,  \nI‚Äôm working on my final-year research paper in AI/Gen-AI/Data Engineering, and I need help choosing the **best advanced research topic** that I can implement using **only free and open-source tools** (no GPT-4, no paid APIs, no proprietary datasets).\n\n# My constraints:\n\n* Must be **advanced enough** to look impressive in research + job interviews\n* Must be doable in **2 months**\n* Must use **100% free tools** (Llama 3, Mistral, Chroma, Qdrant, FAISS, HuggingFace, PyTorch, LangChain, AutoGen, CrewAI, etc.)\n* The topic should NOT depend on paid GPT models or have a paid model that performs significantly better\n* Should help for roles like **AI Engineer, Gen-AI Engineer, ML Engineer, or Data Engineer**\n\n# Topics I‚Äôm considering:\n\n1. **RAG Optimization Using Open-Source LLMs** ‚Äì Hybrid search, advanced chunking, long-context models, vector DB tuning\n2. **Vector Database Index Optimization** ‚Äì Evaluating HNSW, IVF, PQ, ScaNN using FAISS/Qdrant/Chroma\n3. **Open-Source Multi-Agent LLM Systems** ‚Äì Using CrewAI/AutoGen with Llama 3/Mistral to build planning & tool-use agents\n4. **Embedding Model Benchmarking for Domain Retrieval** ‚Äì Comparing E5, bge-large, mpnet, SFR, MiniLM for semantic search tasks\n5. **Context Compression for Long-Context LLMs** ‚Äì Implementing summarization + reranking + filtering pipelines\n\n# What I need advice on:\n\n* Which topic gives the **best job-market advantage**?\n* Which one is realistically doable in **2 months** by one person?\n* Which topic has the strongest **open-source ecosystem**, with no need for GPT-4?\n* Which topic has the best potential for a strong research paper?\n\nAny suggestions or personal experience would be really appreciated!  \nThanks!",
    "score": 3,
    "ups": 3,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 13:44:32",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2t3pr/looking_for_advice_best_advanced_ai_topic_for/"
  },
  {
    "id": "1p2sm00",
    "title": "Need help /contributors for a project concerned with fl-sam-lora upon fed-kits",
    "selftext": "Need help for this project I don't know what to do ",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 13:13:14",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2sm00/need_help_contributors_for_a_project_concerned/"
  },
  {
    "id": "1p2s8ri",
    "title": "Need help /contributors for a project concerned with fl-sam-lora upon fed-kits",
    "selftext": "",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 12:48:59",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2s8ri/need_help_contributors_for_a_project_concerned/"
  },
  {
    "id": "1p2r3mv",
    "title": "Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning (New Foundation Model)",
    "selftext": "We at Lexsi Labs are excited to introduce¬†**Orion-MSP**, a new tabular foundation model designed for¬†*in-context learning on structured data*. It leverages multi-scale sparse attention and Perceiver-style memory to model tabular datasets at multiple granularities.\n\n**Key components:**\n\n* **Multi-Scale Sparse Attention**¬†for windowed, global, and random patterns\n* **Perceiver-style compressed memory**¬†for cross-component information flow\n* **Hierarchical feature modeling**¬†for both local and global context\n\nOrion-MSP is built to be efficient, near-linear in complexity, and robust across datasets with varying feature counts.\n\nOpen-source code, experiments, and feedback welcome.",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 11:39:33",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2r3mv/orionmsp_multiscale_sparse_attention_for_tabular/"
  },
  {
    "id": "1p2myf3",
    "title": "Nvidia GPU for deep learning",
    "selftext": "Hi, I am trying to invest into NVIDIA GPU's for deep learning, I am doing a few projects and looking for card. I looked at two options the Nvidia RTX 5070 Ti (16GB) and Nvidia RTX 4000 Ada (20GB). The stuff I am attempting to do is Self-Supervised Learning (SSL) for Images and a regular image segmentation project. I know both of these cards arnt ideal cause SSL needs large batch size which need a lot of memory. But I am trying to manage with budget I have (for the entire desktop, I dont want to spend more than 6k AUD and there are some options in Lenova etc).\n\nWhat I want to find out is what is the main difference between the two cards, I know 5070 Ti (16GB) is much newer architecture. What I hear is the RTX 4000 Ada (20GB) is old so wanted to find out if anyone knows about it performance. I am inclined to go for 4000 Ada because of the extra 4GB VRAM.\n\nAlso if there any alternatives (better cards) please let me know.",
    "score": 7,
    "ups": 7,
    "downs": 0,
    "comments": 12,
    "created_utc": "2025-11-21 08:08:37",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2myf3/nvidia_gpu_for_deep_learning/"
  },
  {
    "id": "1p2mp1q",
    "title": "Cant improve Accuracy more than 81%",
    "selftext": "Help guide me on how to improve Accuracy for cnn models",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 07:56:34",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2mp1q/cant_improve_accuracy_more_than_81/"
  },
  {
    "id": "1p2lm6z",
    "title": "Theory for Karpathy's \"Zero to Hero\"",
    "selftext": "I always enjoyed \"understanding\" how LLMs work but never actually implemented it. After a friend recommended \"zero to hero\", I have been hooked!!\n\nI am just 1.5 videos in, but still feel there are gaps in what I am learning. I am also implementing the code myself along with watching.\n\nI took an ML class in my college but its been 8 years and I don't remember much.\n\nHe mentions some topics like \"cross entropy loss\", \"learning rate decay\" or \"maximum likelihood estimation\", but don't necessarily go in depth. I want to structure my learnings more.\n\nCan someone please suggest reading material to read along with these videos or some pre-requisites? I do not want to fall in tutorial trap.",
    "score": 16,
    "ups": 16,
    "downs": 0,
    "comments": 10,
    "created_utc": "2025-11-21 07:06:08",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2lm6z/theory_for_karpathys_zero_to_hero/"
  },
  {
    "id": "1p2l3x4",
    "title": "GravOpt under constant attack ‚Äì still reaches ground state (real-time demo)",
    "selftext": "Azuro AI + GravOpt ‚Äì Bulgarian quantum-inspired optimization platform\n\n\n\n\\- 99.9999% MAX-CUT (beats 30-year theoretical bound)\n\n\\- Live demo where the optimizer is under active attack and still wins\n\n\\- Visual multi-domain platform (energy, logistics, finance, biology)\n\n\n\nRepo + sabotage GIF: [https://github.com/Kretski/GravOptAdaptiveE](https://github.com/Kretski/GravOptAdaptiveE)\n\nPro lifetime ‚Ç¨200 (first 100) ‚Äì DM if interested",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 06:42:51",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2l3x4/gravopt_under_constant_attack_still_reaches/"
  },
  {
    "id": "1p2kcft",
    "title": "[Tutorial] DINOv3 with RetinaNet Head for Object Detection",
    "selftext": "DINOv3 with RetinaNet Head for Object Detection\n\n[https://debuggercafe.com/dinov3-with-retinanet-head-for-object-detection/](https://debuggercafe.com/dinov3-with-retinanet-head-for-object-detection/)\n\nThis article is a continuation of the DINOv3 series. This is an incremental post on the lines of object detection using DINOv3 backbone. While in the last article, we used the SSD head for object detection with DINOv3, in this one, we will improve upon it by adding the capability for the RetinaNet head as well. We will carry out both training and inference with DINOv3 with RetinaNet head for object detection.\n\nhttps://preview.redd.it/sfhb13toai2g1.png?width=1000&format=png&auto=webp&s=a27fea0931971f0c1024e17676c608c1fcbc8a9a\n\n",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 06:09:31",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2kcft/tutorial_dinov3_with_retinanet_head_for_object/"
  },
  {
    "id": "1p2haj3",
    "title": "Stop using 1536 dims. Voyage 3.5 Lite @ 512 beats OpenAI Small (and saves 3x RAM)",
    "selftext": "I‚Äôve been optimizing a RAG pipeline while working on [myclone.is](http://myclone.is) recently and found a massive efficiency win that I wanted to share. If you are still using the default¬†text-embedding-3-small¬†(1536 dims), you can likely improve your retrieval quality while slashing our Vector DB storage by¬†**\\~66%**.  \n\n\nIn voice interfaces, latency is the enemy. We were previously using OpenAI‚Äôs¬†text-embedding-3-small¬†(1536 dimensions), but we recently migrated to¬†**Voyage 3.5 Lite**¬†truncated to¬†**512 dimensions**.\n\nThe results were immediate and measurable.\n\n# The Impact on [MyClone.is](http://MyClone.is) \n\nBy reducing the dimensionality from 1536 to 512, we saw massive speed gains in the retrieval step without sacrificing accuracy:\n\n* **RAG Retrieval Latency:**¬†Reduced by¬†**50%**. (Smaller vectors = faster cosine similarity search and lighter payload).\n* **End-to-End Voice Latency:**¬†The total time from \"user speaks\" to \"AI responds\" dropped by¬†**15%**.\n\nFor anyone building real-time RAG (especially Voice), I highly recommend testing this. That 15% shaved off the total turnaround time makes the conversation feel much more natural.\n\nHas anyone else experimented with sub-768-dimension embeddings for low-latency apps?\n\n  \n\n\n",
    "score": 5,
    "ups": 5,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 03:59:49",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2haj3/stop_using_1536_dims_voyage_35_lite_512_beats/"
  },
  {
    "id": "1p2c5u2",
    "title": "What's the best way to sell high quality synthetic data in 2025-26 ?",
    "selftext": "",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 00:44:40",
    "subreddit": "deeplearning",
    "url": "https://www.reddit.com/r/deeplearning/comments/1p2c5u2/whats_the_best_way_to_sell_high_quality_synthetic/"
  },
  {
    "id": "1p2vgay",
    "title": "[D] Looking for mentord",
    "selftext": "Hi everyone! I‚Äôm at an early-stage AI startup. I‚Äôm looking for a mentor who can guide me because I‚Äôm stuck at a stage where I want to go deeper into research-level AI (RL, world models, representation learning, Disentangled/causal), but I‚Äôm not sure how to structure my learning or choose the right direction.\n\nI‚Äôve been studying codebases, world model papers,  but I feel I‚Äôm missing the ‚Äúresearch mindset‚Äù and I‚Äôm not able to convert my curiosity into a project/day-to-day work. I really need someone experienced who can help me with:\n\nHow to align my projects with real research intrest\n\nHow to build towards PhD-level work while working in industry\n\nHelp me understand what is important pushing bechmarks Or exploring novel research ideas\n\n\nIf anyone here is open to mentoring (even occasionally) or can guide me with the right steps, it would mean a lot.\nThank you!",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 16:14:14",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2vgay/d_looking_for_mentord/"
  },
  {
    "id": "1p2v7ak",
    "title": "[D] Vision Transformers and positional encoding: Padding the ALIBI tensor to account for the CLS token?",
    "selftext": "Working on visual transformers for images, now experimenting with positional encoding in the form of \"Attention with Linear Biase\" (ALIBI, \\[1\\], more specifically 2D-ALIBI \\[2\\]).\n\nSay our image is cut in 3-by-3, resulting in 9 patches. Ignoring batch and head dimensions for simplicity.\n\na) Each patch is linearly projected, then the <cls> token is concatenated, resulting in a tensor of (10, embedding size). Computing the scaled dot product attention eventually results in a tensor of (10, 10).\n\nb) ALIBI is meant to provide bias (essentially distance metrics) in the form of a (9, 9) tensor, indicating the distance from each patch to all patches including itself.\n\nThe scaled dot product attention (10, 10) shall be summed to the ALIBI bias (9, 9) before computing the softmax, however they do not share the same dimension.\n\nIs it correct to pad the leftmost column and topmost row of ALIBI with zeros, to account for the <cls> token being able to attend to all patches with a distance of zero, thereby constructing a tensor with shape (10, 10) ?\n\n\\[1\\] Ofir et al., Train short, test long ([https://arxiv.org/pdf/2108.12409](https://arxiv.org/pdf/2108.12409))\n\n\\[2\\] Fuller et al., CROMA ([https://arxiv.org/pdf/2311.00566](https://arxiv.org/pdf/2311.00566))",
    "score": 5,
    "ups": 5,
    "downs": 0,
    "comments": 1,
    "created_utc": "2025-11-21 15:58:49",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2v7ak/d_vision_transformers_and_positional_encoding/"
  },
  {
    "id": "1p2sr2k",
    "title": "[D] Has any system based on Deep Learning ever produced a navigation algorithm which can compete with the manually-designed algorithms , such as particle SLAM?",
    "selftext": "Has any system based on Deep Learning ever produced a navigation algorithm which can compete with the manually-designed algorithms , such as particle SLAM?\n\nI ask because some tech CEOs and their underlings are recently claiming that Deep Learning is omnipotent and can take society directly through The Singularity.  Deep Learning has no weaknesses which cannot be overcome by simply scaling parameter counts, and that \"scaling works\", and  Ilya Sutskever saying \"you have to believe\". Then of course, I have to slog through armies of reddit parrots who repeat these claims ad nauseam on this platform all day.   \n\nJust wanted to see if some professional Machine Learning experts can set the record straight on this.    Where is the robust spatial navigation algorithms that defeats SLAM,  leveraging only big training data and compute -- as Richard Sutton describes in his [\"Bitter Lesson\"](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) ??     \n\nIs such a DL-based navigation algorithm \"five years away\" ??    Just asking questions.  Just putting that out there. Just planting some seeds of discussion.",
    "score": 24,
    "ups": 24,
    "downs": 0,
    "comments": 4,
    "created_utc": "2025-11-21 13:22:33",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2sr2k/d_has_any_system_based_on_deep_learning_ever/"
  },
  {
    "id": "1p2q999",
    "title": "[D] Question regarding CS Phd admission",
    "selftext": "Hi all,\n\nI recently published a paper in ICLR datasets and benchmarking track and it got positive reviews, i enjoyed the research process and im thinking of applying for phd programs in t30 universities in usa. However i come from a tier 3 college in india and the paper i published is self advised; i didnt have anyone to guide me/advise me through. And i dont know any well known researchers who can write me a recommendation letter. How do i tackle this issue? Im specifically interested in areas such as - building data, resource efficient llms, Tiny llms, model compression and data augmentation for better llm performance. I have some people i want to be advised by but they are all in either t30 in usa or top universities in Europe or china. How can i get admitted?",
    "score": 0,
    "ups": 0,
    "downs": 0,
    "comments": 8,
    "created_utc": "2025-11-21 10:51:09",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2q999/d_question_regarding_cs_phd_admission/"
  },
  {
    "id": "1p2ob4h",
    "title": "[D] AAMAS 2026 paper reviews out soon",
    "selftext": "The reviews would be out soon. Rebuttal Period: Nov 21-Nov 25\n\nCreating a thread for the discussion ",
    "score": 17,
    "ups": 17,
    "downs": 0,
    "comments": 2,
    "created_utc": "2025-11-21 09:10:40",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2ob4h/d_aamas_2026_paper_reviews_out_soon/"
  },
  {
    "id": "1p2m7ck",
    "title": "[D] ICLR rebuttal submission deadline",
    "selftext": "Hey everyone, I wanted to ask you what is the deadline to submit rebuttals on the open review for ICLR. Because i am in UK and my time right now is 2:01 am 20th November.\n\nCan you submit like tomorrow afternoon UK time ?",
    "score": 4,
    "ups": 4,
    "downs": 0,
    "comments": 10,
    "created_utc": "2025-11-21 07:33:41",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2m7ck/d_iclr_rebuttal_submission_deadline/"
  },
  {
    "id": "1p2hc7j",
    "title": "[D] New results on ARC 1+2 challenge, overfitting?",
    "selftext": "Never heard about this company, Poetiq, apparently their system used gemini 3.0 and was able to get accuracy to above human baseline levels. Crazy if true. Waiting for confirmation from ARC people.\n\nSource: [https://poetiq.ai/posts/arcagi\\_announcement/](https://poetiq.ai/posts/arcagi_announcement/)\n\nThe github shows some of the tricks they used, to be honest it looks a little like overfitting, there are numpy transformation hardcoded into the prompts: [https://github.com/poetiq-ai/poetiq-arc-agi-solver/blob/main/arc\\_agi/prompts.py](https://github.com/poetiq-ai/poetiq-arc-agi-solver/blob/main/arc_agi/prompts.py)\n\nSeems slightly against the spirit of the challenge since it is encoding specific priors to beat it.  \n**Did you think this is fair? Will the ARC people have to re-formulate what is considered a solution?**  \n",
    "score": 23,
    "ups": 23,
    "downs": 0,
    "comments": 11,
    "created_utc": "2025-11-21 04:01:37",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2hc7j/d_new_results_on_arc_12_challenge_overfitting/"
  },
  {
    "id": "1p2g1q2",
    "title": "[D] Extropic TSU for Probabilistic Neuron Activation in Predictive Coding Algorithm",
    "selftext": "I had an idea today and please correct me if I am wrong.\n\nFrom what I understand, the TSU generates probabilities through controlled stochastic noise which is controlled by voltage. Now assuming that these are cores and their probabilities can be controlled then can't we use each core as a neuron that activates or doesn't activate by determining a value such as 0.571 to calculate the neccasary voltage required to simulate a 57.1% chance for activation within the TSU core?\n\nNow if we do this Back propagation becomes an issue, but what if we ditch it completely? What if we use [Predictive Coding](https://youtu.be/l-OLgbdZ3kk?si=KpxCSq9gXXwWGBsZ&t=426) algorithm which will be continiously trained on this hardware. In short: the predictive coding algorithm is basically Layer1 predicting Layer2 which the errors for Layer1 is stored at Layer2. Due to its simplicity and the efficiency of the hardware it can be run in real time.\n\nNow the memory will be an issue, but that's why we continously train the model to update the neurons to the current task by feeding the relavant information from memory. That way the Neural network continiously learns and adapts to new tasks with little energy in real time.\n\nI believe that if the TSU is a success, then this method could be used to generate a step towards AGI.",
    "score": 1,
    "ups": 1,
    "downs": 0,
    "comments": 0,
    "created_utc": "2025-11-21 03:11:54",
    "subreddit": "MachineLearning",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1p2g1q2/d_extropic_tsu_for_probabilistic_neuron/"
  }
]