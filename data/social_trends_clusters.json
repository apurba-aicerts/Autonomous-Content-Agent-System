[
  {
    "cluster_name": "Deep Learning and Course Updates",
    "titles": [
      "[N] Stanford is updating their Deep Learning course on YouTube"
    ]
  },
  {
    "cluster_name": "Open Source and Contributions",
    "titles": [
      "[D] Open source projects to contribute to as an ML research scientist"
    ]
  },
  {
    "cluster_name": "Researcher Practices and Concerns",
    "titles": [
      "[D] Is it normal for a CV/ML researcher with ~600 citations and h-index 10 to have ZERO public code at all?",
      "[D] How much should researchers (especially in ML domain) rely on LLMs for their work?"
    ]
  },
  {
    "cluster_name": "Algorithm and Model Analysis",
    "titles": [
      "[D] Reverse-engineering Flash Attention 4",
      "[D] Blog Post: 6 Things I hate about SHAP as a Maintainer",
      "[D] Will fine-tuning LLaMA 3.2 11B Instruct on text-only data degrade its vision capabilities?",
      "[D] Tensorflow and Musicnn",
      "[D] Baseline model for Anomaly Detection"
    ]
  },
  {
    "cluster_name": "Job Market and Career",
    "titles": [
      "[D] The job market is weird",
      "Internship at 'Big Tech' — PhD Student [D]",
      "[D] Monthly Who's Hiring and Who wants to be Hired?",
      "[P] I am building a ML job board"
    ]
  },
  {
    "cluster_name": "LLM and AI Research",
    "titles": [
      "[D] I’m looking for papers, preprints, datasets, or reports where an LLM is trained to only know what humans knew before a major scientific breakthrough, and is then asked to propose a new theoretical frameworkwithout using post-breakthrough knowledge and without requiring experimental validation.",
      "[R] No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping",
      "[R] New paper shows that draws in LLM battles aren't what you think",
      "[R] New paper: LLMs don't have privileged self knowledge, which means we can efficiently train a General Correctness Model to predict the correctness of multiple models. Surprising or expected?",
      "[D] LLM Inference on TPUs",
      "[R] Thesis direction: mechanistic interpretability vs semantic probing of LLM reasoning?",
      "[D] Anyone here using LLM-as-a-Judge for agent evaluation?"
    ]
  },
  {
    "cluster_name": "Academic Conferences and Grants",
    "titles": [
      "[D] AAAI 26 Social Impact Track",
      "[D] Looking for travel grant sources for NeurIPS 2025 — any leads?",
      "[D] Student Travel Grant for EMNLP",
      "[D] ICLR submission numbers?",
      "[D] KDD 2026 Reviews"
    ]
  },
  {
    "cluster_name": "Model Training and Techniques",
    "titles": [
      "[D] join pretraining or posttraining",
      "[D] Model parallel training use cases",
      "[D] Training a Vision model on a Text-Only Dataset using Axolotl",
      "[D] Experiences with active learning for real applications?",
      "[D]How do you balance pushing new models vs optimizing what you already have?"
    ]
  },
  {
    "cluster_name": "Innovative Tools and Interfaces",
    "titles": [
      "[P] ExoSeeker: A Web Interface For Building Custom Stacked Models For Exoplanet Classifications",
      "[P] Harmonic Agent: Tackling belief drift in self-reflective AI agents",
      "[P] chess-cv: CNN-based chess piece classifier"
    ]
  },
  {
    "cluster_name": "Hardware and Deployment",
    "titles": [
      "[D] M4 Mac Mini 16GB vs 5700x+2070super",
      "[P] Model needs to be deployed"
    ]
  },
  {
    "cluster_name": "Miscellaneous Discussions",
    "titles": [
      "[D] Self-Promotion Thread",
      "[D] How To Pitch MetaHeuritsic Techniques to Stakeholders",
      "[D] Simple Questions Thread"
    ]
  },
  {
    "cluster_name": "Dataset and Data Challenges",
    "titles": [
      "[D] Help needed on Train Bogey Dataset",
      "[D] Multi-market retail dataset for computer vision - 1M images, temporally organised by year"
    ]
  },
  {
    "cluster_name": "Theoretical and Mathematical Ideas",
    "titles": [
      "[R] Maths PhD student - Had an idea on diffusion",
      "[R] A Predictive Approach To Enhance Time-Series Forecasting"
    ]
  },
  {
    "cluster_name": "Social and Ethical Impacts",
    "titles": [
      "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention"
    ]
  },
  {
    "cluster_name": "Audio and Labeling Projects",
    "titles": [
      "[P] Looking to interview people who’ve worked on audio labeling for ML (PhD research project)"
    ]
  }
]